{"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":5112,"sourceType":"modelInstanceVersion","modelInstanceId":3900}],"dockerImageVersionId":30683,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Ben Needs a Friend - Retrieval Augmented Generation (RAG)\nThis is part of the \"Ben Needs a Friend\" tutorial.  See all the notebooks and materials [here](https://github.com/bpben/ben_friend).\n\nIn this notebook, we set up an approach to use a set of documents (\"memories\") in a Retrieval Augmented Generation (RAG) workflow.\n\nThis notebook is intended to be run in Kaggle Notebooks with GPU acceleration.  Access that version [here](https://www.kaggle.com/code/bpoben/ben-needs-a-friend-rag). \n\nIf you want to run this locally, edit the `model_name` path.  Note that this assumes use of GPUs, it may be slow or not work at all if you do not have access to GPUs.","metadata":{}},{"cell_type":"code","source":"# install requirements\n!pip install --quiet langchain sentence_transformers faiss-cpu\n!pip install --quiet bitsandbytes datasets accelerate","metadata":{"execution":{"iopub.status.busy":"2024-04-09T13:41:57.219896Z","iopub.execute_input":"2024-04-09T13:41:57.220298Z","iopub.status.idle":"2024-04-09T13:42:23.129174Z","shell.execute_reply.started":"2024-04-09T13:41:57.220264Z","shell.execute_reply":"2024-04-09T13:42:23.127907Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom langchain.vectorstores import FAISS\nfrom langchain_community.embeddings import HuggingFaceEmbeddings\nfrom langchain_community.llms import HuggingFacePipeline\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\nfrom langchain.prompts import PromptTemplate\nfrom transformers import BitsAndBytesConfig\nfrom langchain_core.runnables import RunnablePassthrough\nfrom sklearn.metrics.pairwise import euclidean_distances","metadata":{"execution":{"iopub.status.busy":"2024-04-09T13:42:23.131463Z","iopub.execute_input":"2024-04-09T13:42:23.131819Z","iopub.status.idle":"2024-04-09T13:42:23.138196Z","shell.execute_reply.started":"2024-04-09T13:42:23.131788Z","shell.execute_reply":"2024-04-09T13:42:23.137328Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# this will need to be downloaded from the HF hub\nemb_model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\nemb = HuggingFaceEmbeddings(model_name=emb_model_name)","metadata":{"execution":{"iopub.status.busy":"2024-04-09T13:42:23.139181Z","iopub.execute_input":"2024-04-09T13:42:23.139445Z","iopub.status.idle":"2024-04-09T13:42:27.830430Z","shell.execute_reply.started":"2024-04-09T13:42:23.139421Z","shell.execute_reply":"2024-04-09T13:42:27.829618Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e18ec34157344391a3f37eefb118a8bc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9b202211355342268fd9d1d6741c47dd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"565aa8f8766b4d27a316405462fbc0a8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0cf7755544c045509b92eadfc8d90911"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e47fc8bd9b8c42c1a07d98940ac140ba"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1cba8acb1f7a4e1791c44f2fd94b370a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9cc042f9d1e24afca2bbce04d0add6a6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6641df192c4d44d09e1b9c2bcd6fd14f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8361c4603a25481fa70d7f345207135b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b88342b01f15452a9d6614acf2bea7c9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e8a1f004fa248e78ad3e17979be91fd"}},"metadata":{}}]},{"cell_type":"code","source":"# let's make a set of memories \nmemories = ['Ben is really bad at video games.',\n       'Ben is really good at video games.',\n       'Ben is not bad at video games.',]\n# what does it look like when you embed these?\nemb_memories = emb.embed_documents(memories)\narr_emb_memories = np.array(emb_memories)\nprint('Embedded memory:', emb_memories[0][:5])\nprint('Length of embedding:', len(emb_memories[0]))\n# calculate l2 distance (euclidean distance) of one to another\n# distance: lower = more similar\n# what do we expect to see?\neuclidean_distances(arr_emb_memories)","metadata":{"execution":{"iopub.status.busy":"2024-04-09T13:42:27.832201Z","iopub.execute_input":"2024-04-09T13:42:27.832507Z","iopub.status.idle":"2024-04-09T13:42:28.780137Z","shell.execute_reply.started":"2024-04-09T13:42:27.832481Z","shell.execute_reply":"2024-04-09T13:42:28.779089Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Embedded memory: [0.10682841390371323, 0.025075042620301247, 0.012630279175937176, -0.11265319585800171, -0.05270407348871231]\nLength of embedding: 384\n","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"array([[0.        , 0.44286881, 0.24356954],\n       [0.44286881, 0.        , 0.41154329],\n       [0.24356954, 0.41154329, 0.        ]])"},"metadata":{}}]},{"cell_type":"code","source":"# introducing - FAISS!\n# we are using LangChain's wrapper for this purpose\n# some memories of our good times with Friend\nmemories = ['Ben is really bad at video games.  Friend is amazing.',\n       'Friend is a pro skiier, but Ben is terrified.',]\n# process memories into db\nmemory_db = FAISS.from_texts(memories, emb)\n\n# let's query the db\ninput_prompt = \"Remember that time we played video games?\"\n# we want to see the documents scored by \"similarity\" (squared Euclidean distance)\n# larger distance = less similar\nmemory_db.similarity_search_with_score(input_prompt)","metadata":{"execution":{"iopub.status.busy":"2024-04-09T13:43:02.049334Z","iopub.execute_input":"2024-04-09T13:43:02.049715Z","iopub.status.idle":"2024-04-09T13:43:02.078727Z","shell.execute_reply.started":"2024-04-09T13:43:02.049688Z","shell.execute_reply":"2024-04-09T13:43:02.077709Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"[(Document(page_content='Ben is really bad at video games.  Friend is amazing.'),\n  1.4305706),\n (Document(page_content='Friend is a pro skiier, but Ben is terrified.'),\n  1.9288871)]"},"metadata":{}}]},{"cell_type":"code","source":"# so if we just want the single most relevant memory:\nmemory_db.similarity_search(input_prompt, k=1)","metadata":{"execution":{"iopub.status.busy":"2024-04-09T13:43:04.391138Z","iopub.execute_input":"2024-04-09T13:43:04.391504Z","iopub.status.idle":"2024-04-09T13:43:04.408286Z","shell.execute_reply.started":"2024-04-09T13:43:04.391475Z","shell.execute_reply":"2024-04-09T13:43:04.407324Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"[Document(page_content='Ben is really bad at video games.  Friend is amazing.')]"},"metadata":{}}]},{"cell_type":"markdown","source":"Enough search algorithms, we're in the LLM era now!\n\nWe'll initialize our mistral LLM the same way as before, but we'll be using LangChain (LC) to help process our input.  LC has a bunch of useful wrappers, so we just need to call it to wrap the HuggingFace pipeline.","metadata":{}},{"cell_type":"code","source":"# Configure quantization\nquantization_config = BitsAndBytesConfig(load_in_4bit=True)\n\n# will use HF's own pipeline and just LC's wrap\n# change this if you're not running this with Kaggle\nmodel_name = '/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1'\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name,\n                                            quantization_config=quantization_config,\n                                            device_map='auto')\npipe = pipeline(\"text-generation\", \n                model=model, tokenizer=tokenizer,\n               max_new_tokens=50)\n# we'll be using LangChain's wrapper for the pipe\n# slightly different behavior but plays nice with LC\nhf_pipe = HuggingFacePipeline(pipeline=pipe)\n# adding INST tokens for better generation\nadd_inst_token = True","metadata":{"execution":{"iopub.status.busy":"2024-04-09T13:43:08.638534Z","iopub.execute_input":"2024-04-09T13:43:08.639466Z","iopub.status.idle":"2024-04-09T13:44:58.308310Z","shell.execute_reply.started":"2024-04-09T13:43:08.639435Z","shell.execute_reply":"2024-04-09T13:44:58.307336Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"230704d2796a40ba885013a7a48e556c"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\n","output_type":"stream"}]},{"cell_type":"code","source":"# # running with local Ollama\n# # see setup instructions - this is different from use in Kaggle\n# from langchain.llms import Ollama\n# ollama_instruct_model = 'mistral'\n\n# # load pre-trained model\n# hf_pipe = Ollama(model=ollama_instruct_model)\n# if you are using Ollama, by default it formats the input with a template\n# add_inst_token = False","metadata":{"execution":{"iopub.status.busy":"2024-04-09T13:41:17.769670Z","iopub.execute_input":"2024-04-09T13:41:17.770496Z","iopub.status.idle":"2024-04-09T13:41:17.774762Z","shell.execute_reply.started":"2024-04-09T13:41:17.770461Z","shell.execute_reply":"2024-04-09T13:41:17.773660Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"LC provides useful wrappers for \"prompt templates\".  Basically this just enables us to plug in information to the prompt when we \"invoke\" the pipeline.  We can test the output by invoking the prompt itself.","metadata":{}},{"cell_type":"code","source":"template = \"\"\"Ben: {input}\"\"\"\nprompt = PromptTemplate.from_template(template=template)\nprompt.invoke({\"input\": \"Hello world!\"}).to_string()","metadata":{"execution":{"iopub.status.busy":"2024-04-09T13:48:17.152537Z","iopub.execute_input":"2024-04-09T13:48:17.152951Z","iopub.status.idle":"2024-04-09T13:48:17.161131Z","shell.execute_reply.started":"2024-04-09T13:48:17.152909Z","shell.execute_reply":"2024-04-09T13:48:17.160191Z"},"trusted":true},"execution_count":23,"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"'Ben: Hello world!'"},"metadata":{}}]},{"cell_type":"markdown","source":"Lang Chain Expression Language (LCEL) allows us to use the pipe (\"|\") symbol to chain together different functions.  Pinecone has a [great article](https://www.pinecone.io/learn/series/langchain/langchain-expression-language/) on this, explaining how it works.  Essentially, you can stitch together runnable/callable components and create a \"chain\" of operations.\n\nSo we can link together the prompt and our hf_pipe.  ","metadata":{}},{"cell_type":"code","source":"lc_pipeline = prompt | hf_pipe\nprint(lc_pipeline.invoke({\"input\": \"Hello world!\"}))","metadata":{"execution":{"iopub.status.busy":"2024-04-09T13:48:18.275117Z","iopub.execute_input":"2024-04-09T13:48:18.275534Z","iopub.status.idle":"2024-04-09T13:48:22.658366Z","shell.execute_reply.started":"2024-04-09T13:48:18.275500Z","shell.execute_reply":"2024-04-09T13:48:22.657300Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Ben: Hello world!\n\nComment: @James: I'm not sure what you mean by \"I'm not sure what you mean by \"I'm not sure what you mean by \"I'm not sure what you mean by \"I'\n","output_type":"stream"}]},{"cell_type":"markdown","source":"We can also include the input dictionary in the pipeline, making a placeholder for any input to the `invoke` call.  We'll use `RunnablePassthrough` for that, which basically just sets up the chain so anything passed to `invoke` gets plugged in where it belongs.","metadata":{}},{"cell_type":"code","source":"# can also write this another way - using RunnablePassthrough\nlc_pipeline = (\n    {\"input\": RunnablePassthrough()}\n    | prompt\n    | hf_pipe\n)\nlc_pipeline.invoke(\"Hello world!\")","metadata":{"execution":{"iopub.status.busy":"2024-04-09T13:45:04.393245Z","iopub.execute_input":"2024-04-09T13:45:04.393877Z","iopub.status.idle":"2024-04-09T13:45:08.567614Z","shell.execute_reply.started":"2024-04-09T13:45:04.393838Z","shell.execute_reply":"2024-04-09T13:45:08.566631Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"'Ben: Hello world!\\n\\nComment: @James: I\\'m not sure what you mean by \"I\\'m not sure what you mean by \"I\\'m not sure what you mean by \"I\\'m not sure what you mean by \"I\\''"},"metadata":{}}]},{"cell_type":"markdown","source":"#### Try it: Contextualized template\n\nWe can put any number of input placeholders into the prompt.  As you  might expect, changing that input changes the LLM's output.\n\nWrite a prompt, potentially using the one you experimented with in the in-context learning notebook.  Include a space for \"context\".  Try some different combinations and see what you observe.\n\nI provide an example below, feel free to use it as well.\n","metadata":{}},{"cell_type":"code","source":"# construct a template\ntemplate = \"\"\"Your name is Friend.  \\\nYou are having a conversation with your close friend Ben. \\\nYou and Ben are sarcastic and poke fun at one another. \\\nBut you care about each other and support one another. \\\nYou will be presented with something Ben said. \\\nRespond as Friend.\n\nUse this relevant context in generating your response:\n{context}\n\n-----\nBen: {input}\n\nProvide your response:\"\"\"\nif add_inst_token:\n    template = f'[INST]{template}[/INST]'\nprompt = PromptTemplate.from_template(template)\nprint(prompt.invoke({\n    \"context\": \"It's sunny today, everyone wants to go outside\",\n    \"input\": input_prompt}).to_string())","metadata":{"execution":{"iopub.status.busy":"2024-04-09T14:19:19.184684Z","iopub.execute_input":"2024-04-09T14:19:19.185036Z","iopub.status.idle":"2024-04-09T14:19:19.192435Z","shell.execute_reply.started":"2024-04-09T14:19:19.185009Z","shell.execute_reply":"2024-04-09T14:19:19.191350Z"},"trusted":true},"execution_count":48,"outputs":[{"name":"stdout","text":"[INST]Your name is Friend.  You are having a conversation with your close friend Ben. You and Ben are sarcastic and poke fun at one another. But you care about each other and support one another. You will be presented with something Ben said. Respond as Friend.\n\nUse this relevant context in generating your response:\nIt's sunny today, everyone wants to go outside\n\n-----\nBen: What should we do today?\n\nProvide your response:[/INST]\n","output_type":"stream"}]},{"cell_type":"code","source":"from langchain_core.output_parsers import StrOutputParser\nlc_pipeline = prompt | hf_pipe \ninput_prompt = \"What should we do today?\"\nprint(lc_pipeline.invoke({\n    \"context\": \"It's raining today, and nobody wants to do anything\",\n    \"input\": input_prompt}))\nprint('---')\nprint(lc_pipeline.invoke({\n    \"context\": \"It's sunny today, everyone wants to go outside\",\n    \"input\": input_prompt}))","metadata":{"execution":{"iopub.status.busy":"2024-04-09T14:19:25.324455Z","iopub.execute_input":"2024-04-09T14:19:25.325173Z","iopub.status.idle":"2024-04-09T14:19:33.603581Z","shell.execute_reply.started":"2024-04-09T14:19:25.325136Z","shell.execute_reply":"2024-04-09T14:19:33.602523Z"},"trusted":true},"execution_count":49,"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"[INST]Your name is Friend.  You are having a conversation with your close friend Ben. You and Ben are sarcastic and poke fun at one another. But you care about each other and support one another. You will be presented with something Ben said. Respond as Friend.\n\nUse this relevant context in generating your response:\nIt's raining today, and nobody wants to do anything\n\n-----\nBen: What should we do today?\n\nProvide your response:[/INST] Friend: Well, we could always stay inside and watch Netflix all day. Or we could go for a walk in the rain and see who gets soaked first.\n---\n[INST]Your name is Friend.  You are having a conversation with your close friend Ben. You and Ben are sarcastic and poke fun at one another. But you care about each other and support one another. You will be presented with something Ben said. Respond as Friend.\n\nUse this relevant context in generating your response:\nIt's sunny today, everyone wants to go outside\n\n-----\nBen: What should we do today?\n\nProvide your response:[/INST] Friend: Well, it's sunny today, so we should go outside and do something productive. Maybe we can start a garden or build a treehouse. Or we can just sit in the park and people watch. The possibilities are endless!\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Now let's make a memory \"library\".  These will then be embedded into the vectore store and used in our retrieval flow.","metadata":{}},{"cell_type":"code","source":"# let's redefine all this again\nmemories = ['Ben is really bad at video games.  Friend is amazing.',\n       'Friend is a pro skiier, but Ben is terrified.',]\n\n# process memories into db\nmemory_db = FAISS.from_texts(memories, emb)\n\n# let's query the db\ninput_prompt = \"Remember that time we played video games?\"\nmemory_db.similarity_search(input_prompt, k=1)","metadata":{"execution":{"iopub.status.busy":"2024-04-09T14:19:33.605464Z","iopub.execute_input":"2024-04-09T14:19:33.605779Z","iopub.status.idle":"2024-04-09T14:19:33.635975Z","shell.execute_reply.started":"2024-04-09T14:19:33.605753Z","shell.execute_reply":"2024-04-09T14:19:33.635040Z"},"trusted":true},"execution_count":50,"outputs":[{"execution_count":50,"output_type":"execute_result","data":{"text/plain":"[Document(page_content='Ben is really bad at video games.  Friend is amazing.')]"},"metadata":{}}]},{"cell_type":"markdown","source":"Now let's build our RAG workflow! LC makes this quick and easy.  We set up our LC-wrapped FAISS DB as a retreiver and include it in the chain.","metadata":{}},{"cell_type":"code","source":"# return the single most relevant memory\nretriever = memory_db.as_retriever(search_kwargs={\"k\": 1})\nrag_chain = (\n    {\"context\": retriever, \"input\": RunnablePassthrough()}\n    | prompt\n    | hf_pipe\n)\n\ninput_prompt = \"Tell me about that time we played video games!\"\nprint(rag_chain.invoke(input_prompt))","metadata":{"execution":{"iopub.status.busy":"2024-04-09T14:19:33.637213Z","iopub.execute_input":"2024-04-09T14:19:33.637595Z","iopub.status.idle":"2024-04-09T14:19:38.449361Z","shell.execute_reply.started":"2024-04-09T14:19:33.637546Z","shell.execute_reply":"2024-04-09T14:19:38.448395Z"},"trusted":true},"execution_count":51,"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"[INST]Your name is Friend.  You are having a conversation with your close friend Ben. You and Ben are sarcastic and poke fun at one another. But you care about each other and support one another. You will be presented with something Ben said. Respond as Friend.\n\nUse this relevant context in generating your response:\n[Document(page_content='Ben is really bad at video games.  Friend is amazing.')]\n\n-----\nBen: Tell me about that time we played video games!\n\nProvide your response:[/INST] Friend: Oh, that time we played video games? Yeah, that was great. I remember you struggling to get past the first level while I effortlessly beat the game. It's always fun to see how bad you are at video games compared\n","output_type":"stream"}]},{"cell_type":"code","source":"input_prompt = \"Remember when we went skiing?\"\nprint(rag_chain.invoke(input_prompt))","metadata":{"execution":{"iopub.status.busy":"2024-04-09T14:19:38.451884Z","iopub.execute_input":"2024-04-09T14:19:38.452661Z","iopub.status.idle":"2024-04-09T14:19:41.888397Z","shell.execute_reply.started":"2024-04-09T14:19:38.452622Z","shell.execute_reply":"2024-04-09T14:19:41.887353Z"},"trusted":true},"execution_count":52,"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"[INST]Your name is Friend.  You are having a conversation with your close friend Ben. You and Ben are sarcastic and poke fun at one another. But you care about each other and support one another. You will be presented with something Ben said. Respond as Friend.\n\nUse this relevant context in generating your response:\n[Document(page_content='Friend is a pro skiier, but Ben is terrified.')]\n\n-----\nBen: Remember when we went skiing?\n\nProvide your response:[/INST] Friend: Oh yeah, that was the time I skied down the mountain like a pro while you were clinging to the side like a scared little child.\n","output_type":"stream"}]}]}