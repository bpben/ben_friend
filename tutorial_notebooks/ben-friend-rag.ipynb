{"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":5112,"sourceType":"modelInstanceVersion","modelInstanceId":3900}],"dockerImageVersionId":30684,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Ben Needs a Friend - Retrieval Augmented Generation (RAG)\nThis is part of the \"Ben Needs a Friend\" tutorial.  See all the notebooks and materials [here](https://github.com/bpben/ben_friend).\n\nIn this notebook, we set up an approach to use a set of documents (\"memories\") in a Retrieval Augmented Generation (RAG) workflow.\n\nThis notebook is intended to be run in Kaggle Notebooks with GPU acceleration.  Access that version [here](https://www.kaggle.com/code/bpoben/ben-needs-a-friend-rag). \n\nIf you want to run this locally, edit the `model_name` path.  Note that this assumes use of GPUs, it may be slow or not work at all if you do not have access to GPUs.","metadata":{}},{"cell_type":"code","source":"# install requirements\n!pip install --quiet langchain sentence_transformers faiss-cpu\n!pip install --quiet bitsandbytes accelerate","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom langchain.vectorstores import FAISS\nfrom langchain_community.embeddings import HuggingFaceEmbeddings\nfrom langchain_community.llms import HuggingFacePipeline\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\nfrom langchain.prompts import PromptTemplate\nfrom transformers import BitsAndBytesConfig\nfrom langchain_core.runnables import RunnablePassthrough\nfrom sklearn.metrics.pairwise import euclidean_distances","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# this will need to be downloaded from the HF hub\nemb_model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\nemb = HuggingFaceEmbeddings(model_name=emb_model_name)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# let's make a set of memories \nmemories = ['Ben is really bad at video games.',\n            'Ben is okay at video games.',\n            'Ben is really good at video games.']\n# what does it look like when you embed these?\nemb_memories = emb.embed_documents(memories)\narr_emb_memories = np.array(emb_memories)\nprint('Embedded memory:', emb_memories[0][:5])\nprint('Length of embedding:', len(emb_memories[0]))\n# calculate l2 distance (euclidean distance) of one to another\n# distance: lower = more similar\n# what do we expect to see?\neuclidean_distances(arr_emb_memories)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# introducing - FAISS!\n# we are using LangChain's wrapper for this purpose\n# some memories of our good times with Friend\nmemories = ['Ben is really bad at video games.  Friend is amazing.',\n       'Friend is a pro skiier, but Ben is terrified.',]\n# process memories into db\nmemory_db = FAISS.from_texts(memories, emb)\n\n# let's query the db\ninput_prompt = \"Remember that time we played video games?\"\n# we want to see the documents scored by \"similarity\" (squared Euclidean distance)\n# larger distance = less similar\nmemory_db.similarity_search_with_score(input_prompt)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# so if we just want the single most relevant memory:\nmemory_db.similarity_search(input_prompt, k=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Enough search algorithms, we're in the LLM era now!\n\nWe'll initialize our mistral LLM the same way as before, but we'll be using LangChain (LC) to help process our input.  LC has a bunch of useful wrappers, so we just need to call it to wrap the HuggingFace pipeline.","metadata":{}},{"cell_type":"code","source":"# Configure quantization\nquantization_config = BitsAndBytesConfig(load_in_4bit=True)\n\n# will use HF's own pipeline and just LC's wrap\n# change this if you're not running this with Kaggle\nmodel_name = '/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1'\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name,\n                                            quantization_config=quantization_config,\n                                            device_map='auto')\npipe = pipeline(\"text-generation\", \n                model=model, tokenizer=tokenizer,\n               max_new_tokens=50)\n# we'll be using LangChain's wrapper for the pipe\n# slightly different behavior but plays nice with LC\nhf_pipe = HuggingFacePipeline(pipeline=pipe)\n# adding INST tokens for better generation\nadd_inst_token = True","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # running with local Ollama\n# # see setup instructions - this is different from use in Kaggle\n# from langchain.llms import Ollama\n# ollama_instruct_model = 'mistral'\n\n# # load pre-trained model\n# hf_pipe = Ollama(model=ollama_instruct_model)\n# if you are using Ollama, by default it formats the input with a template\n# add_inst_token = False","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"LC provides useful wrappers for \"prompt templates\".  Basically this just enables us to plug in information to the prompt when we \"invoke\" the pipeline.  We can test the output by invoking the prompt itself.","metadata":{}},{"cell_type":"code","source":"template = \"\"\"Ben: {input}\"\"\"\nprompt = PromptTemplate.from_template(template=template)\nprompt.invoke({\"input\": \"Hello world!\"}).to_string()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lang Chain Expression Language (LCEL) allows us to use the pipe (\"|\") symbol to chain together different functions.  Pinecone has a [great article](https://www.pinecone.io/learn/series/langchain/langchain-expression-language/) on this, explaining how it works.  Essentially, you can stitch together runnable/callable components and create a \"chain\" of operations.\n\nSo we can link together the prompt and our hf_pipe.  ","metadata":{}},{"cell_type":"code","source":"lc_pipeline = prompt | hf_pipe\nprint(lc_pipeline.invoke({\"input\": \"Hello world!\"}))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can also include the input dictionary in the pipeline, making a placeholder for any input to the `invoke` call.  We'll use `RunnablePassthrough` for that, which basically just sets up the chain so anything passed to `invoke` gets plugged in where it belongs.","metadata":{}},{"cell_type":"code","source":"# can also write this another way - using RunnablePassthrough\nlc_pipeline = (\n    {\"input\": RunnablePassthrough()}\n    | prompt\n    | hf_pipe\n)\nlc_pipeline.invoke(\"Hello world!\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Try it: Contextualized template\n\nWe can put any number of input placeholders into the prompt.  As you  might expect, changing that input changes the LLM's output.\n\nWrite a prompt, potentially using the one you experimented with in the in-context learning notebook.  Include a space for \"context\".  Try some different combinations and see what you observe.\n\nI provide an example below, feel free to use it as well.\n","metadata":{}},{"cell_type":"code","source":"# construct a template\ntemplate = \"\"\"Your name is Friend.  \\\nYou are having a conversation with your close friend Ben. \\\nYou and Ben are sarcastic and poke fun at one another. \\\nBut you care about each other and support one another. \\\nYou will be presented with something Ben said. \\\nRespond as Friend.\n\nUse this relevant context in generating your response:\n{context}\n\n-----\nBen: {input}\n\nProvide your response:\"\"\"\nif add_inst_token:\n    template = f'[INST]{template}[/INST]'\nprompt = PromptTemplate.from_template(template)\nprint(prompt.invoke({\n    \"context\": \"It's sunny today, everyone wants to go outside\",\n    \"input\": input_prompt}).to_string())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from langchain_core.output_parsers import StrOutputParser\nlc_pipeline = prompt | hf_pipe \ninput_prompt = \"What should we do today?\"\nprint(lc_pipeline.invoke({\n    \"context\": \"It's raining today, and nobody wants to do anything\",\n    \"input\": input_prompt}))\nprint('---')\nprint(lc_pipeline.invoke({\n    \"context\": \"It's sunny today, everyone wants to go outside\",\n    \"input\": input_prompt}))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let's make a memory \"library\".  These will then be embedded into the vectore store and used in our retrieval flow.","metadata":{}},{"cell_type":"code","source":"# let's redefine all this again\nmemories = ['Ben is really bad at video games.  Friend is amazing.',\n       'Friend is a pro skiier, but Ben is terrified.',]\n\n# process memories into db\nmemory_db = FAISS.from_texts(memories, emb)\n\n# let's query the db\ninput_prompt = \"Remember that time we played video games?\"\nmemory_db.similarity_search(input_prompt, k=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let's build our RAG workflow! LC makes this quick and easy.  We set up our LC-wrapped FAISS DB as a retreiver and include it in the chain.","metadata":{}},{"cell_type":"code","source":"# return the single most relevant memory\nretriever = memory_db.as_retriever(search_kwargs={\"k\": 1})\nrag_chain = (\n    {\"context\": retriever, \"input\": RunnablePassthrough()}\n    | prompt\n    | hf_pipe\n)\n\ninput_prompt = \"Tell me about that time we played video games!\"\nprint(rag_chain.invoke(input_prompt))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_prompt = \"Remember when we went skiing?\"\nprint(rag_chain.invoke(input_prompt))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}