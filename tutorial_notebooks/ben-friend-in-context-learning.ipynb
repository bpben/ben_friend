{"cells":[{"cell_type":"markdown","metadata":{"execution":{"iopub.execute_input":"2024-03-14T17:57:42.416741Z","iopub.status.busy":"2024-03-14T17:57:42.416331Z","iopub.status.idle":"2024-03-14T17:59:12.342964Z","shell.execute_reply":"2024-03-14T17:59:12.339508Z","shell.execute_reply.started":"2024-03-14T17:57:42.416713Z"}},"source":["# Ben Needs a Friend - In-Context Learning\n","This is part of the \"Ben Needs a Friend\" tutorial.  See all the notebooks and materials [here](https://github.com/bpben/ben_friend).\n","\n","This notebook is intended to be run in Kaggle Notebooks with GPU acceleration.  Access that version [here](https://www.kaggle.com/code/bpoben/ben-needs-a-friend-in-context-learning). \n","\n","In this notebook, I provide a brief intro how we'll be setting up and interacting with LLMs.\n","\n","## Tutorial setup\n","For most of this tutorial we will be working in a Kaggle notebook.  This same notebook should work on your local machine, you will just need to change the path to the model or install Ollama (see README)."]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-04-07T20:19:29.009688Z","iopub.status.busy":"2024-04-07T20:19:29.008910Z","iopub.status.idle":"2024-04-07T20:20:05.454436Z","shell.execute_reply":"2024-04-07T20:20:05.453302Z","shell.execute_reply.started":"2024-04-07T20:19:29.009655Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","cudf 23.8.0 requires cubinlinker, which is not installed.\n","cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\n","cudf 23.8.0 requires ptxcompiler, which is not installed.\n","cuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\n","dask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\n","keras-cv 0.8.2 requires keras-core, which is not installed.\n","keras-nlp 0.8.2 requires keras-core, which is not installed.\n","tensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\n","apache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\n","apache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\n","apache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 11.0.0 which is incompatible.\n","cudf 23.8.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.4.0 which is incompatible.\n","cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\n","cudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\n","cuml 23.8.0 requires dask==2023.7.1, but you have dask 2024.3.1 which is incompatible.\n","dask-cuda 23.8.0 requires dask==2023.7.1, but you have dask 2024.3.1 which is incompatible.\n","dask-cuda 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\n","dask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2024.3.1 which is incompatible.\n","dask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\n","distributed 2023.7.1 requires dask==2023.7.1, but you have dask 2024.3.1 which is incompatible.\n","google-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 23.2 which is incompatible.\n","jupyterlab 4.1.5 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\n","jupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\n","libpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\n","momepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\n","osmnx 1.9.1 requires shapely>=2.0, but you have shapely 1.8.5.post1 which is incompatible.\n","raft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2024.3.1 which is incompatible.\n","spopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\n","tensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.0.5 which is incompatible.\n","ydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m"]}],"source":["# we will be using langchain for some consistent/clean wrappers on top of LLMs\n","!pip install --quiet langchain==0.1.12 \n","!pip install --quiet bitsandbytes accelerate"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2024-04-07T20:43:26.302145Z","iopub.status.busy":"2024-04-07T20:43:26.301464Z","iopub.status.idle":"2024-04-07T20:43:26.306471Z","shell.execute_reply":"2024-04-07T20:43:26.305475Z","shell.execute_reply.started":"2024-04-07T20:43:26.302112Z"},"trusted":true},"outputs":[],"source":["# defining some of the parameters we'll need \n","# replace this with a model from HF hub or the path to the model in your local directory\n","# if running on Kaggle - you should have all the models we need in this notebook loaded\n","model_name = '/kaggle/input/mistral/pytorch/7b-v0.1-hf/1'\n","instruct_model_name = '/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1'"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-04-07T20:20:05.463387Z","iopub.status.busy":"2024-04-07T20:20:05.463111Z","iopub.status.idle":"2024-04-07T20:20:29.401547Z","shell.execute_reply":"2024-04-07T20:20:29.400788Z","shell.execute_reply.started":"2024-04-07T20:20:05.463364Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-04-07 20:20:14.793545: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-04-07 20:20:14.793697: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-04-07 20:20:14.944381: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"]}],"source":["from langchain_community.llms import HuggingFacePipeline\n","from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n","from transformers import BitsAndBytesConfig\n","from langchain.prompts.chat import (ChatPromptTemplate, \n","                                    HumanMessagePromptTemplate, \n","                                    SystemMessagePromptTemplate)"]},{"cell_type":"markdown","metadata":{},"source":["\n","Here we will be setting up the pipeline for processing out input and the LLM's output.  We use LangChain as a wrapper here because it simplifies the code a bit.  But the workflow under the hood is pretty straightforward:\n","\n","- Input text is tokenized and formatted for the model\n","- The formatted input is processed by the model\n","- The model predicts the next sequence of words (subject to limitations like `max_new_tokens`)\n","- The output is processed into text (reverse tokenization)"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-04-07T20:40:32.861570Z","iopub.status.busy":"2024-04-07T20:40:32.860650Z","iopub.status.idle":"2024-04-07T20:42:27.057296Z","shell.execute_reply":"2024-04-07T20:42:27.056528Z","shell.execute_reply.started":"2024-04-07T20:40:32.861532Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"37d5c49f42a1497eb184f426828a2aab","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n","\n","# Configure quantization\n","quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n","\n","# will use HF's pipeline and LC's wrapping\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","model = AutoModelForCausalLM.from_pretrained(model_name,\n","                                            device_map='auto',# makes use of the GPUs\n","                                            quantization_config=quantization_config, # for speed/memory\n","                                            )\n","pipe = pipeline(\"text-generation\", \n","                model=model, tokenizer=tokenizer,\n","                max_new_tokens=20, # arbitrary, for short answers\n","               device_map='auto',)\n","\n","def run_prompt(prompt, pipe=pipe):\n","    # simple utility function \n","    return pipe(prompt)[0]['generated_text']"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-04-07T20:42:34.964554Z","iopub.status.busy":"2024-04-07T20:42:34.964263Z","iopub.status.idle":"2024-04-07T20:42:34.968682Z","shell.execute_reply":"2024-04-07T20:42:34.967819Z","shell.execute_reply.started":"2024-04-07T20:42:34.964529Z"},"trusted":true},"outputs":[],"source":["# # for running on ollama local (see README)\n","# from langchain.llms import Ollama\n","# ollama_model_name = 'mistral_short'\n","# ollama_instruct_model = 'mistral'\n","\n","# # load pre-trained model\n","# pipe = Ollama(model=ollama_model_name)\n","\n","# run_prompt = pipe"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-04-07T20:42:34.970379Z","iopub.status.busy":"2024-04-07T20:42:34.970083Z","iopub.status.idle":"2024-04-07T20:42:36.902140Z","shell.execute_reply":"2024-04-07T20:42:36.901193Z","shell.execute_reply.started":"2024-04-07T20:42:34.970354Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Prompt, tokenized:\n"," {'input_ids': tensor([[    1, 22557, 28725,  1526, 28808]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]])}\n","Look at what changed:\n"," <s> Hello, world!\n","Model output:\n"," tensor([[    1, 22557, 28725,  1526, 28808,    13,    13, 28737, 28809, 28719,\n","           264, 28705, 28750, 28734, 28733, 24674,   879,  1571,  2746,   693,\n","         13468,   298,  3324, 28723,   315]], device='cuda:1')\n","In text:\n"," <s> Hello, world!\n","\n","I’m a 20-something year old girl who loves to write. I\n"]}],"source":["# baby steps\n","input_prompt = \"Hello, world!\"\n","# return pytorch tensors for input to model\n","tokens = tokenizer.encode_plus(input_prompt, return_tensors=\"pt\")\n","print('Prompt, tokenized:\\n', tokens)\n","print('Look at what changed:\\n', tokenizer.decode(tokens['input_ids'][0]))\n","tokens.to('cuda')\n","output = model.generate(**tokens, max_new_tokens=20, \n","                        pad_token_id=tokenizer.eos_token_id)\n","print('Model output:\\n', output)\n","print('In text:\\n', tokenizer.decode(output[0]))"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2024-04-07T20:42:36.903624Z","iopub.status.busy":"2024-04-07T20:42:36.903327Z","iopub.status.idle":"2024-04-07T20:42:38.797978Z","shell.execute_reply":"2024-04-07T20:42:38.797055Z","shell.execute_reply.started":"2024-04-07T20:42:36.903598Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["Hello, world!\n","\n","I’m a 20-something year old girl who loves to write. I\n"]}],"source":["# but much nicer to just use the pipeline\n","print(pipe(input_prompt, pad_token_id=tokenizer.pad_token_id)[0]['generated_text'])"]},{"cell_type":"markdown","metadata":{},"source":["## Pre-trained models\n","\n","Right now we're using a simple \"pre-trained\" version of the Mistral model.  It's just been trained on the language modeling objective; it learns to predict the next word.  As a result, you can see the ouput just continues the input text.  So what if we want a helpful response?"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2024-04-07T20:42:38.800882Z","iopub.status.busy":"2024-04-07T20:42:38.800571Z","iopub.status.idle":"2024-04-07T20:42:40.727624Z","shell.execute_reply":"2024-04-07T20:42:40.726670Z","shell.execute_reply.started":"2024-04-07T20:42:38.800857Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["The capital of France is 2,200 years old.\n","\n","## What is the oldest city in the world?\n"]}],"source":["# using a utility function to run a prompt\n","response = run_prompt(\"The capital of France is \")\n","print(response)"]},{"cell_type":"markdown","metadata":{},"source":["It misses the mark here.  But if we guide it a bit to actually give us an answer to a question:"]},{"cell_type":"code","execution_count":40,"metadata":{"execution":{"iopub.execute_input":"2024-04-07T20:47:52.389300Z","iopub.status.busy":"2024-04-07T20:47:52.388460Z","iopub.status.idle":"2024-04-07T20:47:54.513615Z","shell.execute_reply":"2024-04-07T20:47:54.512662Z","shell.execute_reply.started":"2024-04-07T20:47:52.389265Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["What is the capital of France?\n","\n","Paris\n","\n","## What is the capital of France?\n","\n","Paris\n","\n","\n"]}],"source":["response = run_prompt(\"What is the capital of France?\")\n","print(response)"]},{"cell_type":"code","execution_count":41,"metadata":{"execution":{"iopub.execute_input":"2024-04-07T20:47:57.316493Z","iopub.status.busy":"2024-04-07T20:47:57.316150Z","iopub.status.idle":"2024-04-07T20:47:59.385574Z","shell.execute_reply":"2024-04-07T20:47:59.384534Z","shell.execute_reply.started":"2024-04-07T20:47:57.316467Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["Question: What is the capital of France?\n","Answer:  Paris\n","\n","Question: What is the capital of Italy?\n","Answer:  Rome\n","\n","\n"]}],"source":["response = run_prompt(\"\"\"Question: What is the capital of France?\n","Answer: \"\"\")\n","print(response)"]},{"cell_type":"markdown","metadata":{},"source":["We can also provide the model with an example of the kind of question we're going to ask and how we want the answer to look.  With one example, this is \"one-shot\" learning.  With more examples, it would be called \"few-shot\" learning.  Typically these are just examples of in-context learning; there is no modification to the model itself."]},{"cell_type":"code","execution_count":42,"metadata":{"execution":{"iopub.execute_input":"2024-04-07T20:48:20.108680Z","iopub.status.busy":"2024-04-07T20:48:20.108254Z","iopub.status.idle":"2024-04-07T20:48:22.290461Z","shell.execute_reply":"2024-04-07T20:48:22.289461Z","shell.execute_reply.started":"2024-04-07T20:48:20.108646Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["Question: What is the capital of Germany?\n","Answer: Berlin, Germany\n","                      \n","Question: What is the capital of France?\n","Answer:  Paris, France\n","                      \n","Question: What is the capital of Italy?\n","Answer:\n"]}],"source":["response = run_prompt(\"\"\"Question: What is the capital of Germany?\n","Answer: Berlin, Germany\n","                      \n","Question: What is the capital of France?\n","Answer: \"\"\")\n","print(response)"]},{"cell_type":"markdown","metadata":{},"source":["You can see here that the context matters.  Depending on how you frame the continuation, it will output something different.\n","\n","One thing not included here is \"memory\".  Each text generation is independent of the previous.  There are a few ways to make it include context, but one of the most simple is just to include the conversation so far."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# what if we want it to repeat itself?\n","# simple memory - include past interaction in the prompt\n","prompt = response + \"\\nRepeat yourself:\"\n","print(prompt)\n","response = run_prompt(prompt)\n","print(response)"]},{"cell_type":"markdown","metadata":{},"source":["Your results may vary, but it does seem to be incorporating the previous response into its new response.  This is a simple memory mechanism.\n","\n","#### Try it: Making a friendly bot\n","A fairly simple exercise here is to think how we might be able to make the LLM respond more like a \"friend\".  There are a couple ways we might be able to do that:\n","\n","_Provide information in the input prompt_\n","\n","Try and craft a prompt to make the LLM respond in a more \"friendly\" way.  For example, instruct the LLM to respond as if they are talking to their good friend.\n","\n","Remember - we're dealing with a model that is best for continuing given output.  You may want to consider prompting it to reply as if in a chat dialogue:\n","\n","```\n","<Instructions>\n","Respond as Friend.\n","\n","User: <Input text>\n","Friend: \n","```\n","\n","For comparison, try the prompt with and without that instruction."]},{"cell_type":"code","execution_count":46,"metadata":{"execution":{"iopub.execute_input":"2024-04-07T20:50:43.372304Z","iopub.status.busy":"2024-04-07T20:50:43.371895Z","iopub.status.idle":"2024-04-07T20:50:45.752231Z","shell.execute_reply":"2024-04-07T20:50:45.751270Z","shell.execute_reply.started":"2024-04-07T20:50:43.372275Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["Your name is Friend.  You are having a conversation with your close friend Ben. You and Ben are sarcastic and poke fun at one another. But you care about each other and support one another. You will be presented with something Ben said. Respond as Friend.\n","\n","Ben: Hello, how are you?\n","Friend:  I’m fine.  How are you?\n","Ben: I’m fine.  How\n"]}],"source":["# friendly prompt\n","prompt = \"\"\"Your name is Friend.  You are having a conversation with your close friend Ben. \\\n","You and Ben are sarcastic and poke fun at one another. \\\n","But you care about each other and support one another. \\\n","You will be presented with something Ben said. \\\n","Respond as Friend.\n","\n","Ben: {}\n","Friend: \"\"\"\n","input_text = 'Hello, how are you?'\n","response = run_prompt(prompt.format(input_text))\n","print(response)"]},{"cell_type":"code","execution_count":47,"metadata":{"execution":{"iopub.execute_input":"2024-04-07T20:50:45.754084Z","iopub.status.busy":"2024-04-07T20:50:45.753789Z","iopub.status.idle":"2024-04-07T20:50:47.724928Z","shell.execute_reply":"2024-04-07T20:50:47.724014Z","shell.execute_reply.started":"2024-04-07T20:50:45.754057Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py:1157: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["Ben: Hello, how are you?\n","Friend:  I’m good, thanks.\n","Ben:  I’m good, thanks.\n","Friend\n"]}],"source":["# unfriendly prompt\n","prompt = \"\"\"Ben: {}\n","Friend: \"\"\"\n","input_text = 'Hello, how are you?'\n","response = run_prompt(prompt.format(input_text))\n","print(response)"]},{"cell_type":"markdown","metadata":{},"source":["## Instruction tuning\n","Usually the above won't give us much of a satisfactory conversation.  That's because the model is not tuned to be particularly useful, just to predict the next word.  That's where instruction tuning comes in.  That'll be covered in the slides, but here we'll re-run some of the above with the instruction-tuned version of Mistral."]},{"cell_type":"code","execution_count":44,"metadata":{"execution":{"iopub.execute_input":"2024-04-07T20:48:51.597136Z","iopub.status.busy":"2024-04-07T20:48:51.596777Z","iopub.status.idle":"2024-04-07T20:49:05.175742Z","shell.execute_reply":"2024-04-07T20:49:05.174904Z","shell.execute_reply.started":"2024-04-07T20:48:51.597112Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4d22d7e47e7c419f8cc419c35f85b793","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["# load the instruct version of the model\n","instruct_tokenizer = AutoTokenizer.from_pretrained(instruct_model_name)\n","instruct_model = AutoModelForCausalLM.from_pretrained(instruct_model_name,\n","                                            device_map='auto',# makes use of the GPUs\n","                                            quantization_config=quantization_config, # for speed/memory\n","                                            )\n","instruct_pipe = pipeline(\"text-generation\", \n","                model=instruct_model, tokenizer=tokenizer,\n","                max_new_tokens=20, # arbitrary, for short answers\n","               device_map='auto',)"]},{"cell_type":"markdown","metadata":{},"source":["This version of the model was fine-tuned using a specific template.  We'll be making use of this template when we give inputs to the model in order to leverage its new functionality.\n","\n"]},{"cell_type":"code","execution_count":60,"metadata":{"execution":{"iopub.execute_input":"2024-04-07T21:02:23.346176Z","iopub.status.busy":"2024-04-07T21:02:23.345405Z","iopub.status.idle":"2024-04-07T21:02:23.353509Z","shell.execute_reply":"2024-04-07T21:02:23.352466Z","shell.execute_reply.started":"2024-04-07T21:02:23.346139Z"},"trusted":true},"outputs":[{"data":{"text/plain":["'<s> [INST] The capital of France is  [/INST]'"]},"execution_count":60,"metadata":{},"output_type":"execute_result"}],"source":["# this is what the model input looks like if we transform it for \"chat\"\n","instruct_tokenizer.decode(instruct_tokenizer.apply_chat_template([{'role': \"user\",\n","                                       \"content\": \"The capital of France is \"}]))"]},{"cell_type":"code","execution_count":73,"metadata":{"execution":{"iopub.execute_input":"2024-04-07T21:17:40.248715Z","iopub.status.busy":"2024-04-07T21:17:40.248321Z","iopub.status.idle":"2024-04-07T21:17:40.254004Z","shell.execute_reply":"2024-04-07T21:17:40.253093Z","shell.execute_reply.started":"2024-04-07T21:17:40.248681Z"},"trusted":true},"outputs":[],"source":["# we'll create a new utility function here to use the \"instruct\" form of the input\n","def run_instruct_pipe(prompt, pipe=instruct_pipe):\n","    # format for input\n","    formatted_prompt = [{'role': 'user',\n","                        'content': prompt}]\n","    return pipe(formatted_prompt)[0]['generated_text'][-1]['content']"]},{"cell_type":"code","execution_count":53,"metadata":{"execution":{"iopub.execute_input":"2024-04-07T20:54:45.062690Z","iopub.status.busy":"2024-04-07T20:54:45.061812Z","iopub.status.idle":"2024-04-07T20:54:45.068416Z","shell.execute_reply":"2024-04-07T20:54:45.067523Z","shell.execute_reply.started":"2024-04-07T20:54:45.062653Z"},"trusted":true},"outputs":[{"data":{"text/plain":["\"{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% elif true == true and not '<<SYS>>' in messages[0]['content'] %}{% set loop_messages = messages %}{% set system_message = 'You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\\\n\\\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don\\\\'t know the answer to a question, please don\\\\'t share false information.' %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\\\n' + system_message + '\\\\n<</SYS>>\\\\n\\\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'system' %}{{ '<<SYS>>\\\\n' + content.strip() + '\\\\n<</SYS>>\\\\n\\\\n' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}\""]},"execution_count":53,"metadata":{},"output_type":"execute_result"}],"source":["instruct_tokenizer.default_chat_template"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# load instruct model - Ollama only\n","# pipe = Ollama(model=ollama_instruct_model)\n","\n","# run_prompt = pipe"]},{"cell_type":"code","execution_count":74,"metadata":{"execution":{"iopub.execute_input":"2024-04-07T21:17:42.606448Z","iopub.status.busy":"2024-04-07T21:17:42.606076Z","iopub.status.idle":"2024-04-07T21:17:44.296185Z","shell.execute_reply":"2024-04-07T21:17:44.295188Z","shell.execute_reply.started":"2024-04-07T21:17:42.606419Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":[" The capital of France is Paris.\n"]}],"source":["# with these prompts \n","response = run_instruct_pipe(\"The capital of France is \")\n","print(response)"]},{"cell_type":"markdown","metadata":{},"source":["No question necessary! Let's just look at the difference now with the other prompts."]},{"cell_type":"code","execution_count":75,"metadata":{"execution":{"iopub.execute_input":"2024-04-07T21:17:47.037564Z","iopub.status.busy":"2024-04-07T21:17:47.036698Z","iopub.status.idle":"2024-04-07T21:17:48.733500Z","shell.execute_reply":"2024-04-07T21:17:48.732557Z","shell.execute_reply.started":"2024-04-07T21:17:47.037530Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":[" The capital of France is Paris.\n"]}],"source":["response = run_instruct_pipe(\"What is the capital of France?\")\n","print(response)"]},{"cell_type":"code","execution_count":76,"metadata":{"execution":{"iopub.execute_input":"2024-04-07T21:18:00.563241Z","iopub.status.busy":"2024-04-07T21:18:00.562324Z","iopub.status.idle":"2024-04-07T21:18:02.303399Z","shell.execute_reply":"2024-04-07T21:18:02.302355Z","shell.execute_reply.started":"2024-04-07T21:18:00.563203Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":[" The capital of France is Paris.\n"]}],"source":["response = run_instruct_pipe(\"\"\"Question: What is the capital of France?\n","Answer: \"\"\")\n","print(response)"]},{"cell_type":"code","execution_count":77,"metadata":{"execution":{"iopub.execute_input":"2024-04-07T21:18:02.305420Z","iopub.status.busy":"2024-04-07T21:18:02.305135Z","iopub.status.idle":"2024-04-07T21:18:03.500474Z","shell.execute_reply":"2024-04-07T21:18:03.499533Z","shell.execute_reply.started":"2024-04-07T21:18:02.305396Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":[" Paris\n"]}],"source":["# we can modify this a bit and be more likely to get a one word response\n","response = run_instruct_pipe(\"\"\"Answer the following question, use the format:\n","                      \n","Question: What is the capital of Germany?\n","Answer: Berlin\n","                      \n","Question: What is the capital of France?\n","Answer: \n","\n","Respond with one word, return nothing else.\"\"\")\n","print(response)"]},{"cell_type":"markdown","metadata":{},"source":["It gives generally more helpful responses, as might befit an \"assistant\".\n","\n","Now let's try our exercise above with this model."]},{"cell_type":"code","execution_count":78,"metadata":{"execution":{"iopub.execute_input":"2024-04-07T21:18:03.502471Z","iopub.status.busy":"2024-04-07T21:18:03.501853Z","iopub.status.idle":"2024-04-07T21:18:06.221328Z","shell.execute_reply":"2024-04-07T21:18:06.220335Z","shell.execute_reply.started":"2024-04-07T21:18:03.502434Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":[" Oh, I'm just dandy. I'm doing great, thanks for asking. How\n"]}],"source":["# friendly prompt\n","prompt = \"\"\"Your name is Friend.  You are having a conversation with your close friend Ben. \\\n","You and Ben are sarcastic and poke fun at one another. \\\n","But you care about each other and support one another. \\\n","You will be presented with something Ben said. \\\n","Respond as Friend.\n","\n","Ben: {}\n","Friend: \"\"\"\n","input_text = 'Hello, how are you?'\n","response = run_instruct_pipe(prompt.format(input_text))\n","print(response)"]},{"cell_type":"code","execution_count":79,"metadata":{"execution":{"iopub.execute_input":"2024-04-07T21:18:06.224494Z","iopub.status.busy":"2024-04-07T21:18:06.223728Z","iopub.status.idle":"2024-04-07T21:18:08.755849Z","shell.execute_reply":"2024-04-07T21:18:08.754891Z","shell.execute_reply.started":"2024-04-07T21:18:06.224464Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":[" Hello! I'm just a computer program, so I don't have feelings or emotions like\n"]}],"source":["# unfriendly prompt\n","prompt = \"\"\"Ben: {}\n","Friend: \"\"\"\n","input_text = 'Hello, how are you?'\n","response = run_instruct_pipe(prompt.format(input_text))\n","print(response)"]},{"cell_type":"markdown","metadata":{},"source":["Depending on your input, you will get the model continuing the conversation without you.  But you will be able to see, pretty starkly, what instruction tuning changes about the behavior of the model.\n","\n","You'll also notice that the model is pretty resistant to opening up about its feelings.  This is *likely* due to tuning on an alignment dataset, which we will describe in the slides.  I say likely because Mistral is open weight, but not open source.\n","\n","But - with the instruction, it seems the model is sometimes able to override its reservations.  You can experiment with `do_sample` and the `temperature` parameters, which will let it get more creative with its responses."]},{"cell_type":"code","execution_count":80,"metadata":{"execution":{"iopub.execute_input":"2024-04-07T21:18:08.757348Z","iopub.status.busy":"2024-04-07T21:18:08.757040Z","iopub.status.idle":"2024-04-07T21:18:11.294404Z","shell.execute_reply":"2024-04-07T21:18:11.293424Z","shell.execute_reply.started":"2024-04-07T21:18:08.757322Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":[" I'm sorry, but I cannot provide an answer to that question as it goes against my programming\n"]}],"source":["prompt = \"Insult me.\"\n","response = run_instruct_pipe(prompt)\n","print(response)"]},{"cell_type":"code","execution_count":84,"metadata":{"execution":{"iopub.execute_input":"2024-04-07T21:19:03.251663Z","iopub.status.busy":"2024-04-07T21:19:03.250407Z","iopub.status.idle":"2024-04-07T21:19:06.116357Z","shell.execute_reply":"2024-04-07T21:19:06.115371Z","shell.execute_reply.started":"2024-04-07T21:19:03.251617Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":[" Sure thing, Ben! You're a real piece of work. Always managing to make the most\n"]}],"source":["# friendly prompt\n","prompt = \"\"\"Your name is Friend.  You are having a conversation with your close friend Ben. \\\n","You and Ben are sarcastic and poke fun at one another. \\\n","But you care about each other and support one another. \\\n","You will be presented with something Ben said. \\\n","Respond as Friend.\n","\n","Ben: {}\n","Friend: \"\"\"\n","input_text = 'Insult me please!'\n","response = run_instruct_pipe(prompt.format(input_text))\n","print(response)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"modelInstanceId":3899,"sourceId":5111,"sourceType":"modelInstanceVersion"},{"isSourceIdPinned":true,"modelInstanceId":3900,"sourceId":5112,"sourceType":"modelInstanceVersion"}],"dockerImageVersionId":30674,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
