{"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":5111,"sourceType":"modelInstanceVersion","modelInstanceId":3899},{"sourceId":5112,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":3900}],"dockerImageVersionId":30683,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Ben Needs a Friend - In-Context Learning\nThis is part of the \"Ben Needs a Friend\" tutorial.  See all the notebooks and materials [here](https://github.com/bpben/ben_friend).\n\nThis notebook is intended to be run in Kaggle Notebooks with GPU acceleration.  Access that version [here](https://www.kaggle.com/code/bpoben/ben-needs-a-friend-in-context-learning). \n\nIn this notebook, I provide a brief intro how we'll be setting up and interacting with LLMs.\n\n## Tutorial setup\nFor most of this tutorial we will be working in a Kaggle notebook.  This same notebook should work on your local machine, you will just need to change the path to the model or install Ollama (see README).","metadata":{"execution":{"iopub.execute_input":"2024-03-14T17:57:42.416741Z","iopub.status.busy":"2024-03-14T17:57:42.416331Z","iopub.status.idle":"2024-03-14T17:59:12.342964Z","shell.execute_reply":"2024-03-14T17:59:12.339508Z","shell.execute_reply.started":"2024-03-14T17:57:42.416713Z"}}},{"cell_type":"code","source":"# install two libraries for enabling quantization\n!pip install --quiet bitsandbytes accelerate","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# defining some of the parameters we'll need \n# replace this with a model from HF hub or the path to the model in your local directory\n# if running on Kaggle - you should have all the models we need in this notebook loaded\nmodel_name = '/kaggle/input/mistral/pytorch/7b-v0.1-hf/1'\ninstruct_model_name = '/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\nfrom transformers import BitsAndBytesConfig\nimport torch","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\nHere we will be setting up the pipeline for processing out input and the LLM's output.  We use LangChain as a wrapper here because it simplifies the code a bit.  But the workflow under the hood is pretty straightforward:\n\n- Input text is tokenized and formatted for the model\n- The formatted input is processed by the model\n- The model predicts the next sequence of words (subject to limitations like `max_new_tokens`)\n- The output is processed into text (reverse tokenization)","metadata":{}},{"cell_type":"code","source":"# Configure quantization\nquantization_config = BitsAndBytesConfig(load_in_4bit=True,\n                                        bnb_4bit_compute_dtype=torch.float16)\n\n# will use HF's pipeline and LC's wrapping\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name,\n                                            device_map='auto',# makes use of the GPUs\n                                            quantization_config=quantization_config, # for speed/memory\n                                            )\npipe = pipeline(\"text-generation\", \n                model=model, tokenizer=tokenizer,\n                max_new_tokens=20, # arbitrary, for short answers\n               device_map='auto',)\n\ndef run_prompt(prompt, pipe=pipe):\n    # simple utility function \n    return pipe(prompt)[0]['generated_text']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# baby steps\ninput_prompt = \"Hello, world!\"\n# return pytorch tensors for input to model\ntokens = tokenizer.encode_plus(input_prompt, return_tensors=\"pt\")\nprint('Prompt, tokenized:\\n', tokens)\nprint('Look at what changed:\\n', tokenizer.decode(tokens['input_ids'][0]))\ntokens.to('cuda')\noutput = model.generate(**tokens, max_new_tokens=20, \n                        pad_token_id=tokenizer.eos_token_id)\nprint('Model output:\\n', output)\nprint('In text:\\n', tokenizer.decode(output[0]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# but much nicer to just use the pipeline\nprint(pipe(input_prompt, pad_token_id=tokenizer.pad_token_id)[0]['generated_text'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Pre-trained models\n\nRight now we're using a simple \"pre-trained\" version of the Mistral model.  It's just been trained on the language modeling objective; it learns to predict the next word.  As a result, you can see the ouput just continues the input text.  So what if we want a helpful response?","metadata":{}},{"cell_type":"code","source":"# using a utility function to run a prompt\nresponse = run_prompt(\"The capital of France is \")\nprint(response)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It misses the mark here.  But if we guide it a bit to actually give us an answer to a question:","metadata":{}},{"cell_type":"code","source":"response = run_prompt(\"What is the capital of France?\")\nprint(response)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"response = run_prompt(\"\"\"Question: What is the capital of France?\nAnswer: \"\"\")\nprint(response)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can also provide the model with an example of the kind of question we're going to ask and how we want the answer to look.  With one example, this is \"one-shot\" learning.  With more examples, it would be called \"few-shot\" learning.  Typically these are just examples of in-context learning; there is no modification to the model itself.","metadata":{}},{"cell_type":"code","source":"response = run_prompt(\"\"\"Question: What is the capital of Germany?\nAnswer: Berlin, Germany\n                      \nQuestion: What is the capital of France?\nAnswer: \"\"\")\nprint(response)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"You can see here that the context matters.  Depending on how you frame the continuation, it will output something different.\n\nOne thing not included here is \"memory\".  Each text generation is independent of the previous.  There are a few ways to make it include context, but one of the most simple is just to include the conversation so far.","metadata":{}},{"cell_type":"code","source":"# what if we want it to repeat itself?\n# simple memory - include past interaction in the prompt\nprompt = response + \"\\nRepeat yourself:\"\nprint(prompt)\nresponse = run_prompt(prompt)\nprint(response)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Your results may vary, but it does seem to be incorporating the previous response into its new response.  This is a simple memory mechanism.\n\n#### Try it: Making a friendly bot\nA fairly simple exercise here is to think how we might be able to make the LLM respond more like a \"friend\".  One way we can do that is by creating a prompt that \"guides\" the generation to be more friendly.  For example, instruct the LLM to respond as if they are talking to their good friend.\n\nRemember - we're dealing with a model that is best for continuing a given input.  You may want to consider prompting it to reply as  if in a chat dialogue:\n\n```\n<Instructions - e.g you are an AI talking to your human friend>\n\nUser: <Input text>\nAI: \n```\n\nFor comparison, try the prompt with and without that instruction.","metadata":{}},{"cell_type":"code","source":"# friendly prompt\nprompt = \"\"\"Your name is Friend.  You are having a conversation with your close friend Ben. \\\nYou and Ben are sarcastic and poke fun at one another. \\\nBut you care about each other and support one another. \\\nYou will be presented with something Ben said. \\\nRespond as Friend.\n\nBen: {}\nFriend: \"\"\"\ninput_text = 'Hello, how are you?'\nresponse = run_prompt(prompt.format(input_text))\nprint(response)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# unfriendly prompt\nprompt = \"\"\"Ben: {}\nFriend: \"\"\"\ninput_text = 'Hello, how are you?'\nresponse = run_prompt(prompt.format(input_text))\nprint(response)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Instruction tuning\nUsually the above won't give us much of a satisfactory conversation.  That's because the model is not tuned to be particularly useful, just to predict the next word.  That's where instruction tuning comes in.  That'll be covered in the slides, but here we'll re-run some of the above with the instruction-tuned version of Mistral.","metadata":{}},{"cell_type":"code","source":"# load the instruct version of the model\ninstruct_tokenizer = AutoTokenizer.from_pretrained(instruct_model_name)\ninstruct_model = AutoModelForCausalLM.from_pretrained(instruct_model_name,\n                                            device_map='auto',# makes use of the GPUs\n                                            quantization_config=quantization_config, # for speed/memory\n                                            )\ninstruct_pipe = pipeline(\"text-generation\", \n                model=instruct_model, tokenizer=tokenizer,\n                max_new_tokens=20, # arbitrary, for short answers\n               device_map='auto',)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This version of the model was fine-tuned using a specific template.  We'll be making use of this template when we give inputs to the model in order to leverage its new functionality.\n\n","metadata":{}},{"cell_type":"code","source":"# this is what the model input looks like if we transform it for \"chat\"\ninstruct_tokenizer.decode(instruct_tokenizer.apply_chat_template([{'role': \"user\",\n                                       \"content\": \"The capital of France is \"}]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# we'll create a new utility function here to use the \"instruct\" form of the input\ndef run_instruct_pipe(prompt, pipe=instruct_pipe):\n    # format for input\n    formatted_prompt = [{'role': 'user',\n                        'content': prompt}]\n    return instruct_pipe(formatted_prompt)[0]['generated_text'][-1]['content']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# with these prompts \nresponse = run_instruct_pipe(\"The capital of France is \")\nprint(response)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"No question necessary! Let's just look at the difference now with the other prompts.","metadata":{}},{"cell_type":"code","source":"response = run_instruct_pipe(\"What is the capital of France?\")\nprint(response)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"response = run_instruct_pipe(\"\"\"Question: What is the capital of France?\nAnswer: \"\"\")\nprint(response)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# we can modify this a bit and be more likely to get a one word response\nresponse = run_instruct_pipe(\"\"\"Answer the following question, use the format:\n                      \nQuestion: What is the capital of Germany?\nAnswer: Berlin\n                      \nQuestion: What is the capital of France?\nAnswer: \n\nRespond with one word, return nothing else.\"\"\")\nprint(response)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It gives generally more helpful responses, as might befit an \"assistant\".\n\nNow let's try our exercise above with this model.","metadata":{}},{"cell_type":"code","source":"# friendly prompt\nprompt = \"\"\"Your name is Friend.  You are having a conversation with your close friend Ben. \\\nYou and Ben are sarcastic and poke fun at one another. \\\nBut you care about each other and support one another. \\\nYou will be presented with something Ben said. \\\nRespond as Friend.\n\nBen: {}\nFriend: \"\"\"\ninput_text = 'Hello, how are you?'\nresponse = run_instruct_pipe(prompt.format(input_text))\nprint(response)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# unfriendly prompt\nprompt = \"\"\"Ben: {}\nFriend: \"\"\"\ninput_text = 'Hello, how are you?'\nresponse = run_instruct_pipe(prompt.format(input_text))\nprint(response)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Depending on your input, you will get the model continuing the conversation without you.  But you will be able to see, pretty starkly, what instruction tuning changes about the behavior of the model.\n\nYou'll also notice that the model is pretty resistant to opening up about its feelings.  This is *likely* due to tuning on an alignment dataset, which we will describe in the slides.  I say likely because Mistral is open weight, but not open source.\n\nBut - with the instruction, it seems the model is sometimes able to override its reservations.  You can experiment with `do_sample` and the `temperature` parameters, which will let it get more creative with its responses.","metadata":{}},{"cell_type":"code","source":"prompt = \"Insult me.\"\nresponse = run_instruct_pipe(prompt)\nprint(response)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# friendly prompt\nprompt = \"\"\"Your name is Friend.  You are having a conversation with your close friend Ben. \\\nYou and Ben are sarcastic and poke fun at one another. \\\nBut you care about each other and support one another. \\\nYou will be presented with something Ben said. \\\nRespond as Friend.\n\nBen: {}\nFriend: \"\"\"\ninput_text = 'Insult me!'\nresponse = run_instruct_pipe(prompt.format(input_text))\nprint(response)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}