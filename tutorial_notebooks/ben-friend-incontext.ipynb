{"cells":[{"cell_type":"markdown","metadata":{"execution":{"iopub.execute_input":"2024-03-14T17:57:42.416741Z","iopub.status.busy":"2024-03-14T17:57:42.416331Z","iopub.status.idle":"2024-03-14T17:59:12.342964Z","shell.execute_reply":"2024-03-14T17:59:12.339508Z","shell.execute_reply.started":"2024-03-14T17:57:42.416713Z"}},"source":["# Ben Needs a Friend - In-Context Learning\n","This is part of the \"Ben Needs a Friend\" tutorial.  See all the notebooks and materials [here](https://github.com/bpben/ben_friend).\n","\n","In this notebook, I provide a brief intro how we'll be setting up and interacting with LLMs.\n","\n","## Tutorial setup\n","For most of this tutorial we will be working in a Kaggle notebook.  This same notebook should work on your local machine, you will just need to change the `model_name` path to the model on your local machine or on HuggingFace."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# we will be using langchain for some consistent/clean wrappers on top of LLMs\n","!pip install --quiet langchain==0.1.12 \n","!pip install --quiet bitsandbytes accelerate"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# defining some of the parameters we'll need \n","# replace this with a model from HF hub or the path to the model in your local directory\n","# if running on Kaggle - you should have all the models we need in this notebook loaded\n","model_name = '/kaggle/input/mistral/pytorch/7b-v0.1-hf/1'\n","instruct_model = '/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1'"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-18T14:25:33.752182Z","iopub.status.busy":"2024-03-18T14:25:33.751912Z","iopub.status.idle":"2024-03-18T14:26:06.036556Z","shell.execute_reply":"2024-03-18T14:26:06.035725Z","shell.execute_reply.started":"2024-03-18T14:25:33.752158Z"},"trusted":true},"outputs":[],"source":["from langchain_community.llms import HuggingFacePipeline\n","from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n","from transformers import BitsAndBytesConfig"]},{"cell_type":"markdown","metadata":{},"source":["\n","Here we will be setting up the pipeline for processing out input and the LLM's output.  We use LangChain as a wrapper here because it simplifies the code a bit.  But the workflow under the hood is pretty straightforward:\n","\n","- Input text is tokenized and formatted for the model\n","- The formatted input is processed by the model\n","- The model predicts the next sequence of words (subject to limitations like `max_new_tokens`)\n","- The output is processed into text (reverse tokenization)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-18T14:26:06.039379Z","iopub.status.busy":"2024-03-18T14:26:06.038812Z","iopub.status.idle":"2024-03-18T14:28:11.236021Z","shell.execute_reply":"2024-03-18T14:28:11.234981Z","shell.execute_reply.started":"2024-03-18T14:26:06.039351Z"},"trusted":true},"outputs":[],"source":["from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n","\n","# Configure quantization\n","quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n","\n","# will use HF's pipeline and LC's wrapping\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","model = AutoModelForCausalLM.from_pretrained(model_name,\n","                                            device_map='auto',# makes use of the GPUs\n","                                            quantization_config=quantization_config, # for speed/memory\n","                                            )\n","pipe = pipeline(\"text-generation\", \n","                model=model, tokenizer=tokenizer,\n","                max_new_tokens=20, # arbitrary, for short answers\n","               device_map='auto',)\n","\n","def run_prompt(prompt, pipe=pipe):\n","    # simple utility function \n","    return pipe(prompt)[0]['generated_text']"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-18T14:28:11.237942Z","iopub.status.busy":"2024-03-18T14:28:11.237538Z","iopub.status.idle":"2024-03-18T14:28:16.445169Z","shell.execute_reply":"2024-03-18T14:28:16.444240Z","shell.execute_reply.started":"2024-03-18T14:28:11.237907Z"},"trusted":true},"outputs":[],"source":["# baby steps\n","input_prompt = \"Hello, world!\"\n","# return pytorch tensors for input to model\n","tokens = tokenizer.encode_plus(input_prompt, return_tensors=\"pt\")\n","print('Prompt, tokenized:\\n', tokens)\n","print('Look at what changed:\\n', tokenizer.decode(tokens['input_ids'][0]))\n","tokens.to('cuda')\n","output = model.generate(**tokens, max_new_tokens=20, \n","                        pad_token_id=tokenizer.eos_token_id)\n","print('Model output:\\n', output)\n","print('In text:\\n', tokenizer.decode(output[0]))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-18T14:28:16.446844Z","iopub.status.busy":"2024-03-18T14:28:16.446408Z","iopub.status.idle":"2024-03-18T14:28:18.345017Z","shell.execute_reply":"2024-03-18T14:28:18.343938Z","shell.execute_reply.started":"2024-03-18T14:28:16.446797Z"},"trusted":true},"outputs":[],"source":["# but much nicer to just use the pipeline\n","print(pipe(input_prompt, pad_token_id=tokenizer.pad_token_id)[0]['generated_text'])"]},{"cell_type":"markdown","metadata":{},"source":["## Pre-trained models\n","\n","Right now we're using a simple \"pre-trained\" version of the Mistral model.  It's just been trained on the language modeling objective; it learns to predict the next word.  As a result, you can see the ouput just continues the input text.  So what if we want a helpful response?"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-18T14:28:18.353885Z","iopub.status.busy":"2024-03-18T14:28:18.353531Z","iopub.status.idle":"2024-03-18T14:28:20.263317Z","shell.execute_reply":"2024-03-18T14:28:20.262355Z","shell.execute_reply.started":"2024-03-18T14:28:18.353860Z"},"trusted":true},"outputs":[],"source":["# using a utility function to run a prompt\n","response = run_prompt(\"The capital of France is \")\n","print(response)"]},{"cell_type":"markdown","metadata":{},"source":["It misses the mark here.  But if we guide it a bit to actually give us an answer to a question:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-18T14:28:20.265240Z","iopub.status.busy":"2024-03-18T14:28:20.264856Z","iopub.status.idle":"2024-03-18T14:28:22.197638Z","shell.execute_reply":"2024-03-18T14:28:22.196525Z","shell.execute_reply.started":"2024-03-18T14:28:20.265205Z"},"trusted":true},"outputs":[],"source":["response = run_prompt(\"What is the capital of France?\")\n","print(response)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-18T14:28:22.201382Z","iopub.status.busy":"2024-03-18T14:28:22.201017Z","iopub.status.idle":"2024-03-18T14:28:24.160679Z","shell.execute_reply":"2024-03-18T14:28:24.159467Z","shell.execute_reply.started":"2024-03-18T14:28:22.201356Z"},"trusted":true},"outputs":[],"source":["response = run_prompt(\"\"\"Question: What is the capital of France?\n","Answer: \"\"\")\n","print(response)"]},{"cell_type":"markdown","metadata":{},"source":["We can also provide the model with an example of the kind of question we're going to ask and how we want the answer to look.  With one example, this is \"one-shot\" learning.  With more examples, it would be called \"few-shot\" learning.  Typically these are just examples of in-context learning; there is no modification to the model itself."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["response = run_prompt(\"\"\"Question: What is the capital of Germany?\n","Answer: Berlin, Germany\n","                      \n","Question: What is the capital of France?\n","Answer: \"\"\")\n","print(response)"]},{"cell_type":"markdown","metadata":{},"source":["You can see here that the context matters.  Depending on how you frame the continuation, it will output something different.\n","\n","One thing not included here is \"memory\".  Each text generation is independent of the previous.  There are a few ways to make it include context, but one of the most simple is just to include the conversation so far."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-18T14:28:24.162811Z","iopub.status.busy":"2024-03-18T14:28:24.162393Z","iopub.status.idle":"2024-03-18T14:28:26.194235Z","shell.execute_reply":"2024-03-18T14:28:26.193310Z","shell.execute_reply.started":"2024-03-18T14:28:24.162767Z"},"trusted":true},"outputs":[],"source":["# what if we want it to repeat itself?\n","# simple memory - include past interaction in the prompt\n","prompt = response + \"\\nRepeat yourself:\"\n","print(prompt)\n","response = run_prompt(prompt)\n","print(response)"]},{"cell_type":"markdown","metadata":{},"source":["Your results may vary, but it does seem to be incorporating the previous response into its new response.  This is a simple memory mechanism.\n","\n","#### Try it: Making a friendly bot\n","A fairly simple exercise here is to think how we might be able to make the LLM respond more like a \"friend\".  There are a couple ways we might be able to do that:\n","\n","_Provide information in the input prompt_\n","\n","Try and craft a prompt to make the LLM respond in a more \"friendly\" way.  For example, instruct the LLM to respond as if they are talking to their good friend.\n","\n","Remember - we're dealing with a model that is best for continuing given output.  You may want to consider prompting it to reply as if in a chat dialogue:\n","\n","```\n","<Instructions>\n","Respond as Friend.\n","\n","User: <Input text>\n","Friend: \n","```\n","\n","For comparison, try the prompt with and without that instruction."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-15T22:04:58.483908Z","iopub.status.busy":"2024-03-15T22:04:58.483542Z","iopub.status.idle":"2024-03-15T22:05:00.891780Z","shell.execute_reply":"2024-03-15T22:05:00.890774Z","shell.execute_reply.started":"2024-03-15T22:04:58.483880Z"},"trusted":true},"outputs":[],"source":["# friendly prompt\n","prompt = \"\"\"Your name is Friend.  You are having a conversation with your close friend Ben. \\\n","You and Ben are sarcastic and poke fun at one another. \\\n","But you care about each other and support one another. \\\n","You will be presented with something Ben said. \\\n","Respond as Friend.\n","\n","Ben: {}\n","Friend: \"\"\"\n","input_text = 'Hello, how are you?'\n","response = run_prompt(prompt.format(input_text))\n","print(response)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-15T22:05:48.509791Z","iopub.status.busy":"2024-03-15T22:05:48.508786Z","iopub.status.idle":"2024-03-15T22:05:50.630454Z","shell.execute_reply":"2024-03-15T22:05:50.629492Z","shell.execute_reply.started":"2024-03-15T22:05:48.509754Z"},"trusted":true},"outputs":[],"source":["# unfriendly prompt\n","prompt = \"\"\"Ben: {}\n","Friend: \"\"\"\n","input_text = 'Hello, how are you?'\n","response = run_prompt(prompt.format(input_text))\n","print(response)"]},{"cell_type":"markdown","metadata":{},"source":["## Instruction tuning\n","Usually the above won't give us much of a satisfactory conversation.  That's because the model is not tuned to be particularly useful, just to predict the next word.  That's where instruction tuning comes in.  That'll be covered in the slides, but here we'll re-run some of the above with the instruction-tuned version of Mistral."]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/Users/bpben/miniconda3/envs/llamabot/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n","  warn_deprecated(\n"]},{"name":"stdout","output_type":"stream","text":[" The capital city of France is Paris. Paris is one of the most famous cities in the world, known for its iconic landmarks such as the Eiffel Tower, the Louvre Museum, and Notre-Dame Cathedral, among other attractions. It is also home to many important cultural institutions and universities, making it a major center for art, fashion, gastronomy, science, and business. Paris has a population of over 10 million people in its metropolitan area, making it the most populous city in France.\n"]}],"source":["# using a utility function to run a prompt\n","response = run_prompt(\"The capital of France is \")\n","print(response)"]},{"cell_type":"markdown","metadata":{},"source":["Whoa! No question necessary!"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["response = run_prompt(\"What is the capital of France?\")\n","print(response)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["response = run_prompt(\"\"\"Question: What is the capital of France?\n","Answer: \"\"\")\n","print(response)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# we can modify this a bit and be more likely to get a one word response\n","response = run_prompt(\"\"\"Answer the following question, use the format:\n","                      \n","Question: What is the capital of Germany?\n","Answer: Berlin\n","                      \n","Question: What is the capital of France?\n","Answer: \n","\n","Respond with one word, return nothing else.\"\"\")\n","print(response)"]},{"cell_type":"markdown","metadata":{},"source":["'Nuff said - it gives generally more helpful responses, as might befit an \"assistant\".\n","\n","Now let's try our exercise above with this model."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# friendly prompt\n","prompt = \"\"\"Your name is Friend.  You are having a conversation with your close friend Ben. \\\n","You and Ben are sarcastic and poke fun at one another. \\\n","But you care about each other and support one another. \\\n","You will be presented with something Ben said. \\\n","Respond as Friend.\n","\n","Ben: {}\n","Friend: \"\"\"\n","input_text = 'Hello, how are you?'\n","response = run_prompt(prompt.format(input_text))\n","print(response)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# unfriendly prompt\n","prompt = \"\"\"Ben: {}\n","Friend: \"\"\"\n","input_text = 'Hello, how are you?'\n","response = run_prompt(prompt.format(input_text))\n","print(response)"]},{"cell_type":"markdown","metadata":{},"source":["Depending on your input, you will get the model continuing the conversation without you.  But you will be able to see, pretty starkly, what instruction tuning changes about the behavior of the model.\n","\n","You'll also notice that the model is pretty resistant to opening up about its feelings.  This is *likely* due to tuning on an alignment dataset, which we will describe in the slides.  I say likely because Mistral is open weight, but not open source.\n","\n","But - with the instruction, it seems the model is sometimes able to override its reservations."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["prompt = \"Insult me.\"\n","run_prompt(prompt)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# friendly prompt\n","prompt = \"\"\"Your name is Friend.  You are having a conversation with your close friend Ben. \\\n","You and Ben are sarcastic and poke fun at one another. \\\n","But you care about each other and support one another. \\\n","You will be presented with something Ben said. \\\n","Respond as Friend.\n","\n","Ben: {}\n","Friend: \"\"\"\n","input_text = 'Insult me.'\n","response = run_prompt(prompt.format(input_text))\n","print(response)"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"databundleVersionId":6745013,"modelInstanceId":3900,"sourceId":5112,"sourceType":"modelInstanceVersion"},{"databundleVersionId":6744792,"modelInstanceId":3899,"sourceId":5111,"sourceType":"modelInstanceVersion"}],"dockerImageVersionId":30664,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"llamabot","language":"python","name":"llamabot"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.6"}},"nbformat":4,"nbformat_minor":4}
