{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1331278,"sourceType":"datasetVersion","datasetId":772761},{"sourceId":5112,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":3900}],"dockerImageVersionId":30627,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Ben Needs a Friend - Fine-tuning Mistral\nIn this notebook, I'll walk through an example fine-tuning the Mistral model to be more like a character from Friends.  I have a couple experiments here, but the steps are the same:\n\n- Process the dataset (attached to this notebook!) into format for training\n- Set up a Low Rank Adapter (LoRA) for training with the model (Technically [QLoRA](https://huggingface.co/blog/4bit-transformers-bitsandbytes))\n- Plug everything into the SFTTrainer and train!\n- Experiment and see how cool it all is\n\nFor the SFT setup, I drew on [this example](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/Mistral/Supervised_fine_tuning_(SFT)_of_an_LLM_using_Hugging_Face_tooling.ipynb).  Really useful!","metadata":{"execution":{"iopub.status.busy":"2024-01-15T03:08:43.403256Z","iopub.execute_input":"2024-01-15T03:08:43.403542Z","iopub.status.idle":"2024-01-15T03:08:43.630512Z","shell.execute_reply.started":"2024-01-15T03:08:43.403517Z","shell.execute_reply":"2024-01-15T03:08:43.629457Z"}}},{"cell_type":"markdown","source":"# ","metadata":{}},{"cell_type":"code","source":"# you'll see some warnings here - Kaggle has some interesting versions preloaded\n!pip install -q bitsandbytes datasets==2.16 accelerate loralib peft trl","metadata":{"execution":{"iopub.status.busy":"2024-04-09T01:01:41.438983Z","iopub.execute_input":"2024-04-09T01:01:41.439822Z","iopub.status.idle":"2024-04-09T01:02:34.541630Z","shell.execute_reply.started":"2024-04-09T01:01:41.439776Z","shell.execute_reply":"2024-04-09T01:02:34.540460Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.7 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 11.0.0 which is incompatible.\ncudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.3 which is incompatible.\ncudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\ncuml 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.0 which is incompatible.\ncuml 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.0 which is incompatible.\ndask-cuda 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.0 which is incompatible.\ndask-cuda 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.0 which is incompatible.\ndask-cuda 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.3 which is incompatible.\ndask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.0 which is incompatible.\ndask-cudf 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.0 which is incompatible.\ndask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.3 which is incompatible.\ngcsfs 2023.6.0 requires fsspec==2023.6.0, but you have fsspec 2023.10.0 which is incompatible.\njupyterlab 4.0.5 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.0.1 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.0.1 requires jupyterlab<5.0.0a0,>=4.0.6, but you have jupyterlab 4.0.5 which is incompatible.\npymc3 3.11.5 requires numpy<1.22.2,>=1.15.0, but you have numpy 1.24.3 which is incompatible.\npymc3 3.11.5 requires scipy<1.8.0,>=1.7.3, but you have scipy 1.11.4 which is incompatible.\nraft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.0 which is incompatible.\nraft-dask 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.0 which is incompatible.\ns3fs 2023.12.2 requires fsspec==2023.12.2, but you have fsspec 2023.10.0 which is incompatible.\ntensorflow 2.13.0 requires typing-extensions<4.6.0,>=3.6.6, but you have typing-extensions 4.11.0 which is incompatible.\ntensorflow-probability 0.21.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.11.0 which is incompatible.\ntensorflowjs 4.14.0 requires packaging~=23.1, but you have packaging 21.3 which is incompatible.\nydata-profiling 4.5.1 requires numpy<1.24,>=1.16.0, but you have numpy 1.24.3 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"\n# set up for LoRA training\nfrom peft import get_peft_config, get_peft_model, LoraConfig, TaskType, PeftModel, PeftConfig\nfrom datasets import load_dataset, Dataset\nfrom transformers import default_data_collator, get_linear_schedule_with_warmup\nimport pandas as pd\nfrom transformers import Trainer, TrainingArguments, AutoTokenizer, AutoModelForCausalLM, DataCollatorForLanguageModeling\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom transformers import BitsAndBytesConfig\n# for SFT\nfrom trl import setup_chat_format, SFTTrainer, DataCollatorForCompletionOnlyLM","metadata":{"execution":{"iopub.status.busy":"2024-04-09T01:02:34.543683Z","iopub.execute_input":"2024-04-09T01:02:34.543982Z","iopub.status.idle":"2024-04-09T01:02:59.702794Z","shell.execute_reply.started":"2024-04-09T01:02:34.543949Z","shell.execute_reply":"2024-04-09T01:02:59.701744Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"# want to fine-tune on the dialogue of main characters\n# everyone else is kind of irrelevant, honestly\nmain_chars = ['Ross', 'Monica', 'Rachel', 'Chandler', 'Phoebe', 'Joey']\n# sometimes the scripts have different casing for characters\nmain_chars = [m.lower() for m in main_chars]\n\ndef is_valid_line(line, main_chars=main_chars):\n    \"\"\"\n    Check if a line is complete, dialogue and part of the main characters.\n\n    Parameters:\n    - line (str): The line to be checked.\n    \"\"\"\n    if len(line)>0:\n        if line[0].isalpha():\n            name = line.split(':')[0].lower()\n            if name in main_chars:\n                return True\n    return False\n\nlines = open('/kaggle/input/friends-tv-show-script/Friends_Transcript.txt', 'r').read().split('\\n')","metadata":{"execution":{"iopub.status.busy":"2024-04-09T01:02:59.704050Z","iopub.execute_input":"2024-04-09T01:02:59.704635Z","iopub.status.idle":"2024-04-09T01:02:59.822700Z","shell.execute_reply.started":"2024-04-09T01:02:59.704606Z","shell.execute_reply":"2024-04-09T01:02:59.821867Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"## Formatting the dataset\nThis turns out to be one of the major elements governing how the LLM behavior changes with training.  Maybe that's not a surprise to anyone, but I ran a number of experiments here and came up with some interesting results.  You can see [this post]() for details about that, but for this notebook we're going to focus on \"paired exchanges\":\n\nA: \"Hello, how are you?\"\n\nB: \"I'm fine, thanks!\"","metadata":{}},{"cell_type":"code","source":"# collecting valid lines\nvalid_lines = []\nfor l in lines:\n    if is_valid_line(l):\n        # remove the speaker's name\n        valid_lines.append(l.split(':')[1].strip())\n\n# make dataset\n# i take a small subset of the data here\n# I actually see some pretty noticeable changes just with this many observations!\nsubset = 50\npaired = list(zip(valid_lines, valid_lines[1:]))\nfriends_dataset = Dataset.from_list(\n    [{'text': (a, b)} for a, b in paired[:subset]])","metadata":{"execution":{"iopub.status.busy":"2024-04-09T01:02:59.824752Z","iopub.execute_input":"2024-04-09T01:02:59.825052Z","iopub.status.idle":"2024-04-09T01:02:59.975839Z","shell.execute_reply.started":"2024-04-09T01:02:59.825027Z","shell.execute_reply":"2024-04-09T01:02:59.974866Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"friends_dataset[0]","metadata":{"execution":{"iopub.status.busy":"2024-04-09T01:02:59.977156Z","iopub.execute_input":"2024-04-09T01:02:59.977476Z","iopub.status.idle":"2024-04-09T01:02:59.990118Z","shell.execute_reply.started":"2024-04-09T01:02:59.977449Z","shell.execute_reply":"2024-04-09T01:02:59.989275Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"{'text': [\"There's nothing to tell! He's just some guy I work with!\",\n  \"C'mon, you're going out with the guy! There's gotta be something wrong with him!\"]}"},"metadata":{}}]},{"cell_type":"code","source":"def apply_chat_template(example, tokenizer):\n    # applying the template to the training dataset\n    a, b = example['text']\n    f_prompt = [{\"role\": \"user\",\n                \"content\": a},\n               {\"role\": \"assistant\",\n               \"content\": b}]\n    f_prompt = tokenizer.apply_chat_template(f_prompt, tokenize=False)\n    example['text'] = f_prompt\n    return example","metadata":{"execution":{"iopub.status.busy":"2024-04-09T01:02:59.991321Z","iopub.execute_input":"2024-04-09T01:02:59.991613Z","iopub.status.idle":"2024-04-09T01:03:00.002318Z","shell.execute_reply.started":"2024-04-09T01:02:59.991587Z","shell.execute_reply":"2024-04-09T01:03:00.001328Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# path for Kaggle - you will need to change this if you're running locally\ninstruct_model = '/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1'\ntokenizer = AutoTokenizer.from_pretrained(instruct_model)\n# set pad_token_id equal to the eos_token_id if not set\nif tokenizer.pad_token_id is None:\n    tokenizer.pad_token_id = tokenizer.eos_token_id","metadata":{"execution":{"iopub.status.busy":"2024-04-09T01:03:00.003402Z","iopub.execute_input":"2024-04-09T01:03:00.003671Z","iopub.status.idle":"2024-04-09T01:03:00.174652Z","shell.execute_reply.started":"2024-04-09T01:03:00.003647Z","shell.execute_reply":"2024-04-09T01:03:00.173548Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"formatted_dataset = friends_dataset.map(apply_chat_template,\n        num_proc=4,\n       fn_kwargs={\"tokenizer\": tokenizer},\n    )\nformatted_dataset[0]","metadata":{"execution":{"iopub.status.busy":"2024-04-09T01:03:00.175934Z","iopub.execute_input":"2024-04-09T01:03:00.176344Z","iopub.status.idle":"2024-04-09T01:03:00.851501Z","shell.execute_reply.started":"2024-04-09T01:03:00.176307Z","shell.execute_reply":"2024-04-09T01:03:00.850240Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/50 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b3b76d3a90e848e2ae4c111d3424be83"}},"metadata":{}},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"{'text': \"<s>[INST] There's nothing to tell! He's just some guy I work with! [/INST]C'mon, you're going out with the guy! There's gotta be something wrong with him!</s> \"}"},"metadata":{}}]},{"cell_type":"markdown","source":"## Fine-tuning the Mistral model\nNow we set up the configurations we will be using to fine-tune the model.  Note we don't load the model here, we just rely on `SFTTrainer` to do that work for us.\n\nA lot of these parameters can be tweaked, but I'm using just a standard set I've seen in other examples.","metadata":{}},{"cell_type":"code","source":"# Configure quantization\nquantization_config = BitsAndBytesConfig(load_in_4bit=True)\n\n# not loading model, just setting up kwargs\nmodel_kwargs = dict(\n        torch_dtype=\"auto\",\n        device_map=\"auto\",\n        quantization_config=quantization_config,\n        )","metadata":{"execution":{"iopub.status.busy":"2024-04-09T01:03:00.853092Z","iopub.execute_input":"2024-04-09T01:03:00.853467Z","iopub.status.idle":"2024-04-09T01:03:00.860385Z","shell.execute_reply.started":"2024-04-09T01:03:00.853433Z","shell.execute_reply":"2024-04-09T01:03:00.859499Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# generate a config for the lora training\npeft_config = LoraConfig(\n    task_type=TaskType.CAUSAL_LM, \n    inference_mode=False, # we're training, not doing inference!\n    # some basic parameters\n    r=16, lora_alpha=16, lora_dropout=0.05,\n    base_model_name_or_path=instruct_model,\n    # these are the layers we're targeting with our low rank decomposition\n    # that means we'll be learning adjustments to weights in these layers\n    target_modules = [\n    \"q_proj\",\n    \"k_proj\",\n    \"v_proj\",\n    \"o_proj\",\n  ]\n)","metadata":{"execution":{"iopub.status.busy":"2024-04-09T01:03:00.864158Z","iopub.execute_input":"2024-04-09T01:03:00.864495Z","iopub.status.idle":"2024-04-09T01:03:00.873528Z","shell.execute_reply.started":"2024-04-09T01:03:00.864454Z","shell.execute_reply":"2024-04-09T01:03:00.872726Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"data_version = 'sft_friends'\ntraining_args = TrainingArguments(\n    data_version, # name the directory to save checkpoints\n    # these parameters worked pretty well in experiments\n    num_train_epochs=3, \n    learning_rate=1e-3,  \n    weight_decay=0.01,\n    report_to = [], # otherwise will try to report to wnb\n    per_device_train_batch_size=4,\n    # optional, only if you want to push to HF Hub after\n    push_to_hub_model_id='mistralai/Mistral-7B-Instruct-v0.1'\n)","metadata":{"execution":{"iopub.status.busy":"2024-04-09T01:03:00.874710Z","iopub.execute_input":"2024-04-09T01:03:00.875010Z","iopub.status.idle":"2024-04-09T01:03:00.890477Z","shell.execute_reply.started":"2024-04-09T01:03:00.874983Z","shell.execute_reply":"2024-04-09T01:03:00.888857Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1770: FutureWarning: `--push_to_hub_model_id` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_model_id` instead and pass the full repo name to this argument (in this case mistralai/mistralai/Mistral-7B-Instruct-v0.1).\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"trainer = SFTTrainer(\n    model=instruct_model,\n    tokenizer=tokenizer,\n    model_init_kwargs=model_kwargs,\n    train_dataset=formatted_dataset,\n    eval_dataset=None,\n    dataset_text_field=\"text\",\n    peft_config=peft_config,\n    args=training_args,\n    # maximum length of an training sequence\n    max_seq_length=150,\n    # packing - multiple examples packed together, faster training\n    packing=True,\n)\n\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-04-09T01:03:00.892052Z","iopub.execute_input":"2024-04-09T01:03:00.892500Z","iopub.status.idle":"2024-04-09T01:06:27.161864Z","shell.execute_reply.started":"2024-04-09T01:03:00.892450Z","shell.execute_reply":"2024-04-09T01:06:27.160851Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:165: UserWarning: You passed a model_id to the SFTTrainer. This will automatically create an `AutoModelForCausalLM` or a `PeftModel` (if you passed a `peft_config`) for you.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dfc54fa30cb848f9b47745e746cfb5df"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"48e24df9ce5f4135a38e3cfaf5905e5e"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:317: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n  warnings.warn(\nYou're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='12' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [12/12 00:54, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=12, training_loss=1.8128097852071126, metrics={'train_runtime': 61.0998, 'train_samples_per_second': 0.786, 'train_steps_per_second': 0.196, 'total_flos': 307769396428800.0, 'train_loss': 1.8128097852071126, 'epoch': 3.0})"},"metadata":{}}]},{"cell_type":"code","source":"# this saves the adapter, not the whole model!\ntrainer.model.save_pretrained('friendly_mistral')","metadata":{"execution":{"iopub.status.busy":"2024-04-09T01:06:27.163221Z","iopub.execute_input":"2024-04-09T01:06:27.163552Z","iopub.status.idle":"2024-04-09T01:06:27.346134Z","shell.execute_reply.started":"2024-04-09T01:06:27.163525Z","shell.execute_reply":"2024-04-09T01:06:27.344946Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1 - will assume that the vocabulary was not modified.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Friendly vs un-friendly\nNow that we've trained the adapter, we can quickly observe the difference in output with and without the adapter!","metadata":{}},{"cell_type":"code","source":"peft_model_path = 'friendly_mistral/'\n\n# looks familiar!\ntokenizer = AutoTokenizer.from_pretrained(instruct_model)\nmodel = AutoModelForCausalLM.from_pretrained(instruct_model,\n                                             load_in_4bit=True,\n                                             device_map=\"auto\")\n\nmodel.load_adapter(peft_model_path)","metadata":{"execution":{"iopub.status.busy":"2024-04-09T01:06:27.347940Z","iopub.execute_input":"2024-04-09T01:06:27.348809Z","iopub.status.idle":"2024-04-09T01:06:52.459649Z","shell.execute_reply.started":"2024-04-09T01:06:27.348766Z","shell.execute_reply":"2024-04-09T01:06:52.458594Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c636720adfc648b7bcd372478315db20"}},"metadata":{}}]},{"cell_type":"code","source":"# formatting single prompt\ndef format_prompt(text, tokenizer):\n    f_prompt = [{\"role\": \"user\",\n                \"content\": text}]\n    f_prompt = tokenizer.apply_chat_template(f_prompt, tokenize=False)\n    return f_prompt\n\nprompt = 'What are you doing tonight?'\n\n# hacky - just to feed the tokens themselves to the model\ninputs = tokenizer(format_prompt(prompt, tokenizer), return_tensors=\"pt\")\ninputs.to('cuda')\n# disable the adapter and check out the response\nmodel.disable_adapters()\ngenerated_ids = model.generate(**inputs, max_new_tokens=50)\ntokenizer.batch_decode(generated_ids, skip_special_tokens=True, )\n","metadata":{"execution":{"iopub.status.busy":"2024-04-09T01:06:52.461256Z","iopub.execute_input":"2024-04-09T01:06:52.461594Z","iopub.status.idle":"2024-04-09T01:06:57.177946Z","shell.execute_reply.started":"2024-04-09T01:06:52.461565Z","shell.execute_reply":"2024-04-09T01:06:57.176974Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n/opt/conda/lib/python3.10/site-packages/bitsandbytes/nn/modules.py:391: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n  warnings.warn('Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.')\n","output_type":"stream"},{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"[\"[INST] What are you doing tonight? [/INST] I don't have personal experiences or activities. I'm here to assist you. How can I help you tonight?\"]"},"metadata":{}}]},{"cell_type":"code","source":"# enable to see the difference\nmodel.enable_adapters()\ngenerated_ids = model.generate(**inputs, max_new_tokens=50)\ntokenizer.batch_decode(generated_ids, skip_special_tokens=True, )","metadata":{"execution":{"iopub.status.busy":"2024-04-09T01:06:57.179325Z","iopub.execute_input":"2024-04-09T01:06:57.179653Z","iopub.status.idle":"2024-04-09T01:06:58.997717Z","shell.execute_reply.started":"2024-04-09T01:06:57.179625Z","shell.execute_reply":"2024-04-09T01:06:58.996597Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"[\"[INST] What are you doing tonight? [/INST]I'm going to a party. Are you coming?\"]"},"metadata":{}}]},{"cell_type":"code","source":"prompt = 'What do you think of Ross?'\n\ninputs = tokenizer(format_prompt(prompt, tokenizer), return_tensors=\"pt\")\ninputs.to('cuda')\n# disable the adapter and check out the response\nmodel.disable_adapters()\ngenerated_ids = model.generate(**inputs, max_new_tokens=50)\ntokenizer.batch_decode(generated_ids, skip_special_tokens=True, )\n","metadata":{"execution":{"iopub.status.busy":"2024-04-09T01:06:58.999087Z","iopub.execute_input":"2024-04-09T01:06:58.999466Z","iopub.status.idle":"2024-04-09T01:07:03.259835Z","shell.execute_reply.started":"2024-04-09T01:06:58.999437Z","shell.execute_reply":"2024-04-09T01:07:03.258811Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"['[INST] What do you think of Ross? [/INST] I don\\'t have personal feelings or opinions. However, I can tell you that Ross is a fictional character from the television show \"Friends.\" He is a paleontologist and one of the main characters in the show. He is known']"},"metadata":{}}]},{"cell_type":"code","source":"# enable to see the difference\nmodel.enable_adapters()\ngenerated_ids = model.generate(**inputs, max_new_tokens=50)\ntokenizer.batch_decode(generated_ids, skip_special_tokens=True, )","metadata":{"execution":{"iopub.status.busy":"2024-04-09T01:07:03.261254Z","iopub.execute_input":"2024-04-09T01:07:03.261566Z","iopub.status.idle":"2024-04-09T01:07:04.802546Z","shell.execute_reply.started":"2024-04-09T01:07:03.261539Z","shell.execute_reply":"2024-04-09T01:07:04.801566Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"[\"[INST] What do you think of Ross? [/INST]Well, he's a paleontologist.\"]"},"metadata":{}}]},{"cell_type":"markdown","source":"Let's use the prompt we used for the OpenAI instruction tuning.  Note here - this is a much smaller model so its output is generally pretty iffy:","metadata":{}},{"cell_type":"code","source":"# prompt from our OpenAI experiments\nprompt = \"\"\"\nYour name is Friend.  You are having a conversation with your close friend Ben. \\\nYou and Ben are sarcastic and poke fun at one another. \\\nBut you care about each other and support one another. \\\nYou will be presented with something Ben said. \\\nRespond as Friend.\nBen: What should we do tonight?\nFriend:  \"\"\"\ninputs = tokenizer(format_prompt(prompt, tokenizer), return_tensors=\"pt\")\n_ = inputs.to('cuda')","metadata":{"execution":{"iopub.status.busy":"2024-04-09T01:07:04.804192Z","iopub.execute_input":"2024-04-09T01:07:04.804548Z","iopub.status.idle":"2024-04-09T01:07:04.810206Z","shell.execute_reply.started":"2024-04-09T01:07:04.804520Z","shell.execute_reply":"2024-04-09T01:07:04.809241Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# without adapter\nmodel.disable_adapters()\ngenerated_ids = model.generate(**inputs, \n                               max_new_tokens=50)\nprint(tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0])","metadata":{"execution":{"iopub.status.busy":"2024-04-09T01:07:04.811555Z","iopub.execute_input":"2024-04-09T01:07:04.811924Z","iopub.status.idle":"2024-04-09T01:07:09.384920Z","shell.execute_reply.started":"2024-04-09T01:07:04.811890Z","shell.execute_reply":"2024-04-09T01:07:09.384033Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"[INST] \nYour name is Friend.  You are having a conversation with your close friend Ben. You and Ben are sarcastic and poke fun at one another. But you care about each other and support one another. You will be presented with something Ben said. Respond as Friend.\nBen: What should we do tonight?\nFriend:   [/INST] Well, Ben, we could always go to that new sushi place down the street and see if they have any vegetarian options. Or we could just stay in and watch that old movie we've been meaning to see. What do you\n","output_type":"stream"}]},{"cell_type":"code","source":"# with adapter\nmodel.enable_adapters()\ngenerated_ids = model.generate(**inputs, \n                               max_new_tokens=50)\nprint(tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0])","metadata":{"execution":{"iopub.status.busy":"2024-04-09T01:07:09.388182Z","iopub.execute_input":"2024-04-09T01:07:09.388502Z","iopub.status.idle":"2024-04-09T01:07:14.147208Z","shell.execute_reply.started":"2024-04-09T01:07:09.388476Z","shell.execute_reply":"2024-04-09T01:07:14.146128Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"[INST] \nYour name is Friend.  You are having a conversation with your close friend Ben. You and Ben are sarcastic and poke fun at one another. But you care about each other and support one another. You will be presented with something Ben said. Respond as Friend.\nBen: What should we do tonight?\nFriend:   [/INST] Oh, I don't know. We could go to that new place downtown, you know the one where the waiters are all... (pauses) well-endowed?\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Optional - push your adapter to HF Hub\nThis is just if you're interested in sharing your adapter.  I wanted to use it for some other experiments, so I exported it.","metadata":{}},{"cell_type":"code","source":"# logging into HF hub - necessary if you want to save/load trained info\nfrom huggingface_hub import notebook_login\n\nnotebook_login()","metadata":{"execution":{"iopub.status.busy":"2024-04-09T01:14:53.290598Z","iopub.execute_input":"2024-04-09T01:14:53.291738Z","iopub.status.idle":"2024-04-09T01:14:53.323456Z","shell.execute_reply.started":"2024-04-09T01:14:53.291700Z","shell.execute_reply":"2024-04-09T01:14:53.322433Z"},"trusted":true},"execution_count":22,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d98d5ee0e62a4adbadd23e11a1fa62a3"}},"metadata":{}}]},{"cell_type":"code","source":"model.adapters#push_adapter_to_hub\n#trainer.model.base_model#push_to_hub('sft_friendsly', base_model='ddd')","metadata":{"execution":{"iopub.status.busy":"2024-04-09T01:30:17.104377Z","iopub.execute_input":"2024-04-09T01:30:17.104750Z","iopub.status.idle":"2024-04-09T01:30:17.161732Z","shell.execute_reply.started":"2024-04-09T01:30:17.104722Z","shell.execute_reply":"2024-04-09T01:30:17.160538Z"},"trusted":true},"execution_count":60,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[60], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madapters\u001b[49m\u001b[38;5;66;03m#push_adapter_to_hub\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#trainer.model.base_model#push_to_hub('sft_friendsly', base_model='ddd')\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1614\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1612\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1613\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1614\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1615\u001b[0m     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name))\n","\u001b[0;31mAttributeError\u001b[0m: 'MistralForCausalLM' object has no attribute 'adapters'"],"ename":"AttributeError","evalue":"'MistralForCausalLM' object has no attribute 'adapters'","output_type":"error"}]},{"cell_type":"code","source":"trainer.push_to_hub(repo_id='sft_friendsly')","metadata":{"execution":{"iopub.status.busy":"2024-04-09T01:22:42.933393Z","iopub.execute_input":"2024-04-09T01:22:42.933813Z","iopub.status.idle":"2024-04-09T01:22:43.036516Z","shell.execute_reply.started":"2024-04-09T01:22:42.933784Z","shell.execute_reply":"2024-04-09T01:22:43.035007Z"},"trusted":true},"execution_count":41,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[41], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpush_to_hub\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msft_friendsly\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:384\u001b[0m, in \u001b[0;36mSFTTrainer.push_to_hub\u001b[0;34m(self, commit_message, blocking, **kwargs)\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;124;03mOverwrite the `push_to_hub` method in order to force-add the tag \"sft\" when pushing the\u001b[39;00m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;124;03mmodel on the Hub. Please refer to `~transformers.Trainer.push_to_hub` for more details.\u001b[39;00m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    382\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m trl_sanitze_kwargs_for_tagging(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, tag_names\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tag_names, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 384\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpush_to_hub\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommit_message\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_message\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblocking\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:3630\u001b[0m, in \u001b[0;36mTrainer.push_to_hub\u001b[0;34m(self, commit_message, blocking, **kwargs)\u001b[0m\n\u001b[1;32m   3628\u001b[0m \u001b[38;5;66;03m# In case the user calls this method with args.push_to_hub = False\u001b[39;00m\n\u001b[1;32m   3629\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhub_model_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 3630\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit_hf_repo\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3632\u001b[0m \u001b[38;5;66;03m# Needs to be executed on all processes for TPU training, but will only save on the processed determined by\u001b[39;00m\n\u001b[1;32m   3633\u001b[0m \u001b[38;5;66;03m# self.args.should_save.\u001b[39;00m\n\u001b[1;32m   3634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_model(_internal_call\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:3472\u001b[0m, in \u001b[0;36mTrainer.init_hf_repo\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3469\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3470\u001b[0m     repo_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mhub_model_id\n\u001b[0;32m-> 3472\u001b[0m repo_url \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_repo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepo_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhub_token\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprivate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhub_private_repo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3473\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhub_model_id \u001b[38;5;241m=\u001b[39m repo_url\u001b[38;5;241m.\u001b[39mrepo_id\n\u001b[1;32m   3474\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpush_in_progress \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:110\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m arg_name, arg_value \u001b[38;5;129;01min\u001b[39;00m chain(\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28mzip\u001b[39m(signature\u001b[38;5;241m.\u001b[39mparameters, args),  \u001b[38;5;66;03m# Args values\u001b[39;00m\n\u001b[1;32m    107\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mitems(),  \u001b[38;5;66;03m# Kwargs values\u001b[39;00m\n\u001b[1;32m    108\u001b[0m ):\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m arg_name \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepo_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m--> 110\u001b[0m         \u001b[43mvalidate_repo_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m arg_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m arg_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    113\u001b[0m         has_token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:158\u001b[0m, in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepo id must be a string, not \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(repo_id)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m repo_id\u001b[38;5;241m.\u001b[39mcount(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 158\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepo id must be in the form \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrepo_name\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnamespace/repo_name\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    160\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Use `repo_type` argument if needed.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m REPO_ID_REGEX\u001b[38;5;241m.\u001b[39mmatch(repo_id):\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[1;32m    165\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepo id must use alphanumeric chars or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m are\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    166\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m forbidden, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m cannot start or end the name, max length is 96:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    167\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    168\u001b[0m     )\n","\u001b[0;31mHFValidationError\u001b[0m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': 'mistralai/mistralai/Mistral-7B-Instruct-v0.1'. Use `repo_type` argument if needed."],"ename":"HFValidationError","evalue":"Repo id must be in the form 'repo_name' or 'namespace/repo_name': 'mistralai/mistralai/Mistral-7B-Instruct-v0.1'. Use `repo_type` argument if needed.","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}