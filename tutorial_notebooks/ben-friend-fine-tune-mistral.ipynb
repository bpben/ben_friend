{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1331278,"sourceType":"datasetVersion","datasetId":772761},{"sourceId":5112,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":3900}],"dockerImageVersionId":30627,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Ben Needs a Friend - Fine-tuning Mistral\nThis is part of the \"Ben Needs a Friend\" tutorial.  See all the notebooks and materials [here](https://github.com/bpben/ben_friend).\n\nThis notebook is intended to be run in Kaggle Notebooks with GPU acceleration.  Access that version [here](https://www.kaggle.com/code/bpoben/ben-needs-a-friend-fine-tuning-mistral). \n\nIn this notebook, I'll walk through an example fine-tuning the Mistral model to be more like a character from Friends.  I have a couple experiments here, but the steps are the same:\n\n- Process the dataset (attached to this notebook!) into format for training\n- Set up a Low Rank Adapter (LoRA) for training with the model (Technically [QLoRA](https://huggingface.co/blog/4bit-transformers-bitsandbytes))\n- Plug everything into the SFTTrainer and train!\n- Experiment and see how cool it all is\n\nFor the SFT setup, I drew on [this example](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/Mistral/Supervised_fine_tuning_(SFT)_of_an_LLM_using_Hugging_Face_tooling.ipynb).  Really useful!","metadata":{"execution":{"iopub.status.busy":"2024-01-15T03:08:43.403256Z","iopub.execute_input":"2024-01-15T03:08:43.403542Z","iopub.status.idle":"2024-01-15T03:08:43.630512Z","shell.execute_reply.started":"2024-01-15T03:08:43.403517Z","shell.execute_reply":"2024-01-15T03:08:43.629457Z"}}},{"cell_type":"markdown","source":"# ","metadata":{}},{"cell_type":"code","source":"# you'll see some warnings here - Kaggle has some interesting versions preloaded\n!pip install -q bitsandbytes datasets==2.16 accelerate peft trl","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# set up for LoRA training\nfrom peft import get_peft_config, get_peft_model, LoraConfig, TaskType, PeftModel, PeftConfig\nfrom datasets import load_dataset, Dataset\nfrom transformers import default_data_collator, get_linear_schedule_with_warmup\nimport pandas as pd\nfrom transformers import Trainer, TrainingArguments, AutoTokenizer, AutoModelForCausalLM, DataCollatorForLanguageModeling\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom transformers import BitsAndBytesConfig\n# for SFT\nfrom trl import setup_chat_format, SFTTrainer, DataCollatorForCompletionOnlyLM","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# want to fine-tune on the dialogue of main characters\n# everyone else is kind of irrelevant, honestly\nmain_chars = ['Ross', 'Monica', 'Rachel', 'Chandler', 'Phoebe', 'Joey']\n# sometimes the scripts have different casing for characters\nmain_chars = [m.lower() for m in main_chars]\n\ndef is_valid_line(line, main_chars=main_chars):\n    \"\"\"\n    Check if a line is complete, dialogue and part of the main characters.\n\n    Parameters:\n    - line (str): The line to be checked.\n    \"\"\"\n    if len(line)>0:\n        if line[0].isalpha():\n            name = line.split(':')[0].lower()\n            if name in main_chars:\n                return True\n    return False\n\nlines = open('/kaggle/input/friends-tv-show-script/Friends_Transcript.txt', 'r').read().split('\\n')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Formatting the dataset\nThis turns out to be one of the major elements governing how the LLM behavior changes with training.  Maybe that's not a surprise to anyone, but I ran a number of experiments here and came up with some interesting results.  You can see [this post]() for details about that, but for this notebook we're going to focus on \"paired exchanges\":\n\nA: \"Hello, how are you?\"\n\nB: \"I'm fine, thanks!\"","metadata":{}},{"cell_type":"code","source":"# collecting valid lines\nvalid_lines = []\nfor l in lines:\n    if is_valid_line(l):\n        # remove the speaker's name\n        valid_lines.append(l.split(':')[1].strip())\n\n# make dataset\n# i take a small subset of the data here\n# I actually see some pretty noticeable changes just with this many observations!\nsubset = 50\npaired = list(zip(valid_lines, valid_lines[1:]))\nfriends_dataset = Dataset.from_list(\n    [{'text': (a, b)} for a, b in paired[:subset]])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"friends_dataset[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def apply_chat_template(example, tokenizer):\n    # applying the template to the training dataset\n    a, b = example['text']\n    f_prompt = [{\"role\": \"user\",\n                \"content\": a},\n               {\"role\": \"assistant\",\n               \"content\": b}]\n    f_prompt = tokenizer.apply_chat_template(f_prompt, tokenize=False)\n    example['text'] = f_prompt\n    return example","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# path for Kaggle - you will need to change this if you're running locally\ninstruct_model = '/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1'\ntokenizer = AutoTokenizer.from_pretrained(instruct_model)\n# set pad_token_id equal to the eos_token_id if not set\nif tokenizer.pad_token_id is None:\n    tokenizer.pad_token_id = tokenizer.eos_token_id","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"formatted_dataset = friends_dataset.map(apply_chat_template,\n        num_proc=4,\n       fn_kwargs={\"tokenizer\": tokenizer},\n    )\nformatted_dataset[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Fine-tuning the Mistral model\nNow we set up the configurations we will be using to fine-tune the model.  Note we don't load the model here, we just rely on `SFTTrainer` to do that work for us.\n\nA lot of these parameters can be tweaked, but I'm using just a standard set I've seen in other examples.","metadata":{}},{"cell_type":"code","source":"# Configure quantization\nquantization_config = BitsAndBytesConfig(load_in_4bit=True)\n\n# not loading model, just setting up kwargs\nmodel_kwargs = dict(\n        torch_dtype=\"auto\",\n        device_map=\"auto\",\n        quantization_config=quantization_config,\n        )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# generate a config for the lora training\npeft_config = LoraConfig(\n    task_type=TaskType.CAUSAL_LM, \n    inference_mode=False, # we're training, not doing inference!\n    # some basic parameters\n    r=16, # rank\n    lora_alpha=16, # scaling parameter for the weights - how strong impact on base weights\n    lora_dropout=0.05, # similar to dropout in NN generally\n    base_model_name_or_path=instruct_model,\n    # these are the layers we're targeting with our low rank decomposition\n    # that means we'll be learning adjustments to weights in these layers\n    target_modules = [\n    \"q_proj\",\n    \"k_proj\",\n    \"v_proj\",\n    \"o_proj\",\n  ]\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_version = 'sft_friends'\ntraining_args = TrainingArguments(\n    data_version, # name the directory to save checkpoints\n    # these parameters worked pretty well in experiments\n    num_train_epochs=3, \n    learning_rate=1e-3,  \n    weight_decay=0.01, # type of regularizatin\n    report_to = [], # otherwise will try to report to wnb\n    per_device_train_batch_size=4,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer = SFTTrainer(\n    model=instruct_model,\n    tokenizer=tokenizer,\n    model_init_kwargs=model_kwargs,\n    train_dataset=formatted_dataset,\n    eval_dataset=None,\n    dataset_text_field=\"text\",\n    peft_config=peft_config,\n    args=training_args,\n    # maximum length of an training sequence\n    max_seq_length=150,\n    # packing - multiple examples packed together, faster training\n    packing=True,\n)\n\ntrainer.train()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# this saves the adapter, not the whole model!\ntrainer.model.save_pretrained('friendly_mistral')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Friendly vs un-friendly\nNow that we've trained the adapter, we can quickly observe the difference in output with and without the adapter!","metadata":{}},{"cell_type":"code","source":"peft_model_path = 'friendly_mistral/'\n\n# looks familiar!\nquantization_config = BitsAndBytesConfig(load_in_4bit=True)\ntokenizer = AutoTokenizer.from_pretrained(instruct_model)\nmodel = AutoModelForCausalLM.from_pretrained(instruct_model,\n                                             quantization_config=quantization_config,\n                                             device_map=\"auto\")\n\nmodel.load_adapter(peft_model_path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# formatting single prompt\ndef format_prompt(text, tokenizer):\n    f_prompt = [{\"role\": \"user\",\n                \"content\": text}]\n    f_prompt = tokenizer.apply_chat_template(f_prompt, tokenize=False)\n    return f_prompt\n\nprompt = 'What are you doing tonight?'\n\n# hacky - just to feed the tokens themselves to the model\ninputs = tokenizer(format_prompt(prompt, tokenizer), return_tensors=\"pt\")\ninputs.to('cuda')\n# disable the adapter and check out the response\nmodel.disable_adapters()\ngenerated_ids = model.generate(**inputs, max_new_tokens=50)\ntokenizer.batch_decode(generated_ids, skip_special_tokens=True, )\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# enable to see the difference\nmodel.enable_adapters()\ngenerated_ids = model.generate(**inputs, max_new_tokens=50)\ntokenizer.batch_decode(generated_ids, skip_special_tokens=True, )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prompt = 'What do you think of Ross?'\n\ninputs = tokenizer(format_prompt(prompt, tokenizer), return_tensors=\"pt\")\ninputs.to('cuda')\n# disable the adapter and check out the response\nmodel.disable_adapters()\ngenerated_ids = model.generate(**inputs, max_new_tokens=50)\ntokenizer.batch_decode(generated_ids, skip_special_tokens=True, )\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# enable to see the difference\nmodel.enable_adapters()\ngenerated_ids = model.generate(**inputs, max_new_tokens=50)\ntokenizer.batch_decode(generated_ids, skip_special_tokens=True, )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's use the prompt we used for the OpenAI instruction tuning.  Note here - this is a much smaller model so its output is generally pretty iffy:","metadata":{}},{"cell_type":"code","source":"# prompt from our OpenAI experiments\nprompt = \"\"\"\nYour name is Friend.  You are having a conversation with your close friend Ben. \\\nYou and Ben are sarcastic and poke fun at one another. \\\nBut you care about each other and support one another. \\\nYou will be presented with something Ben said. \\\nRespond as Friend.\nBen: What should we do tonight?\nFriend:  \"\"\"\ninputs = tokenizer(format_prompt(prompt, tokenizer), return_tensors=\"pt\")\n_ = inputs.to('cuda')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# without adapter\nmodel.disable_adapters()\ngenerated_ids = model.generate(**inputs, \n                               max_new_tokens=50)\nprint(tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# with adapter\nmodel.enable_adapters()\ngenerated_ids = model.generate(**inputs, \n                               max_new_tokens=50)\nprint(tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}