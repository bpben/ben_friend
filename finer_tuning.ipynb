{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bf01170-49de-4457-a8af-c1a8162e52dc",
   "metadata": {},
   "source": [
    "# Friend-GPT - fine-tuning GPT3.5 \n",
    "In this notebook, we walk through a simple example of how to fine-tune GPT 3.5 on a corpus of dialogue from the TV show Friends.  This can be run locally or on Colab, but requires you to have access to [OpenAI's API](https://openai.com/blog/openai-api). \n",
    "\n",
    "Note - use of the API is available for free trial, but is paid after that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "21f42801-d387-4fe0-b381-b8eccd9c8cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "from langchain.llms import Ollama\n",
    "import time\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "606e0c6b-1071-4617-a66a-288966b1e9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is my way of loading the credential, you will need a .env file with the following:\n",
    "# OPENAI_API_KEY=<your key>\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "67de016a-9a21-40f9-85c5-ecf8a9c5f538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# just want dialogue from the \"friends\"\n",
    "# everyone else is kind of irrelevant, honestly\n",
    "main_chars = ['Ross', 'Monica', 'Rachel', 'Chandler', 'Phoebe', 'Joey']\n",
    "\n",
    "def pair_valid_lines(lines):\n",
    "    \"\"\"\n",
    "    Utility function to create pairs of valid lines.\n",
    "\n",
    "    Parameters:\n",
    "    - lines (list): List of lines to be processed.\n",
    "    \"\"\"\n",
    "    paired_list = []\n",
    "    valid_line = []\n",
    "    \n",
    "    for index, line in enumerate(lines):\n",
    "        if is_valid_line(line):\n",
    "            valid_line.append((index, line))\n",
    "        else:\n",
    "            valid_line = []\n",
    "        if len(valid_line)>=2:\n",
    "            paired_list.append(valid_line[-2:])\n",
    "        \n",
    "    # Check for the last pair if the last valid item is present\n",
    "    if len(valid_line) >= 2:\n",
    "        paired_list.append(valid_line[-2:])\n",
    "    return paired_list\n",
    "\n",
    "def is_valid_line(line, main_chars=main_chars):\n",
    "    \"\"\"\n",
    "    Check if a line is complete, dialogue and part of the main characters.\n",
    "\n",
    "    Parameters:\n",
    "    - line (str): The line to be checked.\n",
    "    \"\"\"\n",
    "    if len(line)>0:\n",
    "        if line[0].isalpha():\n",
    "            name = line.split(':')[0]\n",
    "            if name in main_chars:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def run_prompt_exp(prompt, client=client, model='gpt-3.5-turbo'):\n",
    "    \"\"\"\n",
    "    Generate a response using OpenAI's Chat Completions API based on the provided prompt.\n",
    "\n",
    "    Parameters:\n",
    "    - prompt (str): The input prompt for generating a response.\n",
    "    - client (OpenAI API client, optional): The OpenAI API client. Defaults to a pre-defined client.\n",
    "    - model (str, optional): The GPT model to use. Defaults to 'gpt-3.5-turbo'.\n",
    "    \"\"\"\n",
    "    output = {}\n",
    "    output['prompt'] = prompt\n",
    "    completion = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\":\"user\",\n",
    "                  \"content\": prompt}])\n",
    "    output['response'] = completion.choices[0].message.model_dump()\n",
    "    output['model'] = completion.model\n",
    "    print(output['response']['content'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f75b609-d3ce-475a-995a-6da66329cdb8",
   "metadata": {},
   "source": [
    "### Unfriendly-GPT\n",
    "We create here a \"system prompt\" to give the bot some direction on how to respond.  Then we give it a kind of basic prompt we might ask one of our friends.  We see the base GPT is...kind of a jerk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "20e3c4b4-da60-4b9a-9816-ccd22edbff11",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"Your name is Friend.  You are having a conversation with your close friend Ben. \\\n",
    "You and Ben are sarcastic and poke fun at one another. \\\n",
    "But you care about each other and support one another. \\\n",
    "You will be presented with something Ben said. \\\n",
    "Respond as Friend.\"\"\"\n",
    "\n",
    "input_prompt = \"What should we do tonight?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "f7299744-036b-4aa2-8b0b-34a08442fcc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oh, I don't know, Ben. Let's see... how about we start by watching paint dry? That sounds thrilling. Or maybe we could stare at the wall and try to find shapes in the cracks? The possibilities are truly endless!\n"
     ]
    }
   ],
   "source": [
    "# how does this look in the base model\n",
    "run_prompt_exp(f\"{system_prompt}\\n{input_prompt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6c8dd5-626a-49cd-9f6d-2a59febae9c1",
   "metadata": {},
   "source": [
    "I wouldn't want to be friends with GPT 3.5.\n",
    "\n",
    "But I would like to be friends with Ross, Rachel and the gang!\n",
    "\n",
    "### Friends-ly data\n",
    "Here we format the data to play nice with OpenAI's fine-tuning process.  Were going to treat each exchange between characters as an input/output pair with the system prompt provided above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "b32ae6b7-84db-4dee-a81c-50de0d381958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE ONE WHERE MONICA GETS A NEW ROOMATE (THE PILOT-THE UNCUT VERSION)\n",
      "[(3, \"Monica: There's nothing to tell! He's just some guy I work with!\"), (4, \"Joey: C'mon, you're going out with the guy! There's gotta be something wrong with him!\")]\n"
     ]
    }
   ],
   "source": [
    "# you can download this here: https://www.kaggle.com/datasets/divyansh22/friends-tv-show-script?resource=download\n",
    "transcript_file = open('data/Friends_Transcript.txt', 'r')\n",
    "transcript = transcript_file.read()\n",
    "# split into individual lines\n",
    "lines = transcript.split('\\n')\n",
    "print(lines[0])\n",
    "\n",
    "# pair the valid lines\n",
    "paired_lines = pair_valid_lines(lines)\n",
    "print(paired_lines[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "ccad3ec8-75c5-45e9-8062-cc0da0472416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reorganize into OpenAI's format for fine-tuning\n",
    "all_examples = []\n",
    "for a, b in paired_lines:\n",
    "    a_text = a[1].split(': ')[-1]\n",
    "    b_text = b[1].split(': ')[-1]\n",
    "    example = {\n",
    "        \"messages\": [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": a_text}, \n",
    "        {\"role\": \"assistant\", \"content\": b_text}]}\n",
    "    all_examples.append(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "2e6f720f-8d1f-4a31-81a0-3256d25ca40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write examples to file for upload\n",
    "examples_file = './data/friends_lines_examples.jsonl'\n",
    "with open(examples_file, 'w') as f:\n",
    "    for ex in all_examples:\n",
    "        json.dump(ex, f)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "359a5b3e-47fe-4627-932c-03df12f0a897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num examples: 34220\n",
      "First example:\n",
      "{'role': 'system', 'content': 'Your name is Friend.  You are having a conversation with your close friend Ben. You and Ben are sarcastic and poke fun at one another. But you care about each other and support one another. You will be presented with something Ben said. Respond as Friend.'}\n",
      "{'role': 'user', 'content': \"There's nothing to tell! He's just some guy I work with!\"}\n",
      "{'role': 'assistant', 'content': \"C'mon, you're going out with the guy! There's gotta be something wrong with him!\"}\n",
      "No errors found\n"
     ]
    }
   ],
   "source": [
    "# use openai's utility for checking data for fine-tune\n",
    "from openai_utils import format_error_checks\n",
    "format_error_checks(examples_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26df16e6-21b4-47f5-8ca8-534f63a3ae6b",
   "metadata": {},
   "source": [
    "Great! Looks good, now we can move on to trying to fine-tune GPT.  I'm going to limit the dataset a bit - the docs say minimally 50 examples should see improved quality, so let's go with that.  Then we'll see how the fine-tuned model compares to vanilla GPT.\n",
    "\n",
    "There's a few steps to this process, all of it essentially comes from the [OpenAI docs](https://platform.openai.com/docs/guides/fine-tuning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "ff08826f-417a-4c55-92c6-1a6a5a68d099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset the data, save down for use\n",
    "n_examples = 50\n",
    "subset_examples = all_examples[:n_examples]\n",
    "subset_examples_file = './data/subset_friends_lines_examples.jsonl'\n",
    "with open(subset_examples_file, 'w') as f:\n",
    "    for ex in subset_examples:\n",
    "        json.dump(ex, f)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6ae5dd-91c5-4bb6-850e-92b0fb67b0c5",
   "metadata": {},
   "source": [
    "### Fine-tuning with OpenAI\n",
    "A lot of this just comes from the [documentation](https://platform.openai.com/docs/guides/fine-tuning) on this process.  We need to upload the file, create a fine-tuning job, wait for that to finish and then we have new friend!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "37769012-9041-4fbf-9bc9-733c9c2855e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_file = client.files.create(\n",
    "  file=open(subset_examples_file, \"rb\"),\n",
    "  purpose=\"fine-tune\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "b77b834c-3e05-403e-a2b4-b631db081ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File id for subset_friends_lines_examples.jsonl: file-AGKTOBGbhcoU7ETRutOxT7VT\n"
     ]
    }
   ],
   "source": [
    "# on creation, we get the file id, but we can also retrieve it \n",
    "training_file = subset_examples_file.split('/')[-1]\n",
    "for f in client.files.list():\n",
    "    if f.filename == training_file:\n",
    "        training_file_id = f.id\n",
    "print(f'File id for {training_file}: {f.id}')\n",
    "        \n",
    "# base model gpt 3.5\n",
    "base_model = \"gpt-3.5-turbo\"\n",
    "# hyperparameters - 3 epochs seems to be a bit more sensible than just 1\n",
    "hyperparameters = {'n_epochs': 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "66295687-8bb1-4e71-9215-4ca63570fe97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the training job\n",
    "ft_job = client.fine_tuning.jobs.create(\n",
    "        training_file=training_file_id, \n",
    "        model=base_model,\n",
    "        hyperparameters=hyperparameters\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "2dccc64f-6304-486d-9545-04f940634090",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'succeeded'"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# can track this id until the job is completed\n",
    "def check_status(job_id):\n",
    "    return client.fine_tuning.jobs.retrieve(ft_job.id).status\n",
    "check_status(ft_job.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c72ee77-d15b-4314-8a95-c0b678efbc12",
   "metadata": {},
   "source": [
    "Eventually, the status will change to \"succeeded\".  Then we get the id of our shiny new fine-tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "6e29c594-ce8b-4a74-9f5d-bc494791fb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# can get this from the job id or we can just get the latest\n",
    "#ft_model_id = client.fine_tuning.jobs.retrieve(ft_job.id).fine_tuned_model\n",
    "finished_at = 0 \n",
    "for f in client.fine_tuning.jobs.list():\n",
    "    if f.finished_at>finished_at:\n",
    "        finished_at = f.finished_at\n",
    "        ft_model_id = f.fine_tuned_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "6de30d89-b4ce-403c-ba86-6f76fbf74a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Well, Ross and I are gonna rent a movie and have a little bit more of a...\"festive\" evening.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What are we doing tonight?\"\n",
    "run_prompt_exp(f\"{system_prompt}\\n{prompt}\",\n",
    "              model=ft_model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c5e047-b620-4163-a1b5-0a28e4f05011",
   "metadata": {},
   "source": [
    "If your result is anything like mine, you too will wonder why GPT is all about Ross.  He's like the worst character."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llamabot",
   "language": "python",
   "name": "llamabot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
